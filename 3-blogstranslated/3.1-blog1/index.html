<!doctype html><html lang=en class="js csstransforms3d"><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.134.3"><meta name=description content><meta name=author content="nhuttse180082@fpt.edu.vn"><link rel=icon href=../../images/favicon.png type=image/png><title>Blog 1 :: Internship Report</title>
<link href=../../css/nucleus.css?1765337914 rel=stylesheet><link href=../../css/fontawesome-all.min.css?1765337914 rel=stylesheet><link href=../../css/hybrid.css?1765337914 rel=stylesheet><link href=../../css/featherlight.min.css?1765337914 rel=stylesheet><link href=../../css/perfect-scrollbar.min.css?1765337914 rel=stylesheet><link href=../../css/auto-complete.css?1765337914 rel=stylesheet><link href=../../css/atom-one-dark-reasonable.css?1765337914 rel=stylesheet><link href=../../css/theme.css?1765337914 rel=stylesheet><link href=../../css/hugo-theme.css?1765337914 rel=stylesheet><link href=../../css/theme-workshop.css?1765337914 rel=stylesheet><script src=../../js/jquery-3.3.1.min.js?1765337914></script><style>:root #header+#content>#left>#rlblock_left{display:none!important}</style></head><body data-url=../../3-blogstranslated/3.1-blog1/><nav id=sidebar class=showVisitedLinks><div id=header-wrapper><div id=header><a id=logo href=../../><svg id="Layer_1" data-name="Layer 1" viewBox="0 0 60 30" width="30%"><defs><style>.cls-1{fill:#fff}.cls-2{fill:#f90;fill-rule:evenodd}</style></defs><title>AWS-Logo_White-Color</title><path class="cls-1" d="M14.09 10.85a4.7 4.7.0 00.19 1.48 7.73 7.73.0 00.54 1.19.77.77.0 01.12.38.64.64.0 01-.32.49l-1 .7a.83.83.0 01-.44.15.69.69.0 01-.49-.23 3.8 3.8.0 01-.6-.77q-.25-.42-.51-1a6.14 6.14.0 01-4.89 2.3 4.54 4.54.0 01-3.32-1.19 4.27 4.27.0 01-1.22-3.2 4.28 4.28.0 011.46-3.4A6.06 6.06.0 017.69 6.46a12.47 12.47.0 011.76.13q.92.13 1.91.36V5.73a3.65 3.65.0 00-.79-2.66A3.81 3.81.0 007.86 2.3a7.71 7.71.0 00-1.79.22 12.78 12.78.0 00-1.79.57 4.55 4.55.0 01-.58.22h-.26q-.35.0-.35-.52V2a1.09 1.09.0 01.12-.58 1.2 1.2.0 01.47-.35A10.88 10.88.0 015.77.32 10.19 10.19.0 018.36.0a6 6 0 014.35 1.35 5.49 5.49.0 011.38 4.09zM7.34 13.38a5.36 5.36.0 001.72-.31A3.63 3.63.0 0010.63 12 2.62 2.62.0 0011.19 11a5.63 5.63.0 00.16-1.44v-.7a14.35 14.35.0 00-1.53-.28 12.37 12.37.0 00-1.56-.1 3.84 3.84.0 00-2.47.67A2.34 2.34.0 005 11a2.35 2.35.0 00.61 1.76A2.4 2.4.0 007.34 13.38zm13.35 1.8a1 1 0 01-.64-.16 1.3 1.3.0 01-.35-.65L15.81 1.51a3 3 0 01-.15-.67.36.36.0 01.41-.41H17.7a1 1 0 01.65.16 1.4 1.4.0 01.33.65l2.79 11 2.59-11A1.17 1.17.0 0124.39.6a1.1 1.1.0 01.67-.16H26.4a1.1 1.1.0 01.67.16 1.17 1.17.0 01.32.65L30 12.39 32.88 1.25A1.39 1.39.0 0133.22.6a1 1 0 01.65-.16h1.54a.36.36.0 01.41.41 1.36 1.36.0 010 .26 3.64 3.64.0 01-.12.41l-4 12.86a1.3 1.3.0 01-.35.65 1 1 0 01-.64.16H29.25a1 1 0 01-.67-.17 1.26 1.26.0 01-.32-.67L25.67 3.64l-2.56 10.7a1.26 1.26.0 01-.32.67 1 1 0 01-.67.17zm21.36.44a11.28 11.28.0 01-2.56-.29 7.44 7.44.0 01-1.92-.67 1 1 0 01-.61-.93v-.84q0-.52.38-.52a.9.9.0 01.31.06l.42.17a8.77 8.77.0 001.83.58 9.78 9.78.0 002 .2 4.48 4.48.0 002.43-.55 1.76 1.76.0 00.86-1.57 1.61 1.61.0 00-.45-1.16A4.29 4.29.0 0043 9.22l-2.41-.76A5.15 5.15.0 0138 6.78a3.94 3.94.0 01-.83-2.41 3.7 3.7.0 01.45-1.85 4.47 4.47.0 011.19-1.37 5.27 5.27.0 011.7-.86A7.4 7.4.0 0142.6.0a8.87 8.87.0 011.12.07q.57.07 1.08.19t.95.26a4.27 4.27.0 01.7.29 1.59 1.59.0 01.49.41.94.94.0 01.15.55v.79q0 .52-.38.52a1.76 1.76.0 01-.64-.2 7.74 7.74.0 00-3.2-.64 4.37 4.37.0 00-2.21.47 1.6 1.6.0 00-.79 1.48 1.58 1.58.0 00.49 1.18 4.94 4.94.0 001.83.92L44.55 7a5.08 5.08.0 012.57 1.6A3.76 3.76.0 0147.9 11a4.21 4.21.0 01-.44 1.93 4.4 4.4.0 01-1.21 1.47 5.43 5.43.0 01-1.85.93A8.25 8.25.0 0142.05 15.62z"/><path class="cls-2" d="M45.19 23.81C39.72 27.85 31.78 30 25 30A36.64 36.64.0 01.22 20.57c-.51-.46-.06-1.09.56-.74A49.78 49.78.0 0025.53 26.4 49.23 49.23.0 0044.4 22.53C45.32 22.14 46.1 23.14 45.19 23.81z"/><path class="cls-2" d="M47.47 21.21c-.7-.9-4.63-.42-6.39-.21-.53.06-.62-.4-.14-.74 3.13-2.2 8.27-1.57 8.86-.83s-.16 5.89-3.09 8.35c-.45.38-.88.18-.68-.32C46.69 25.8 48.17 22.11 47.47 21.21z"/></svg></a></div><div class=searchbox><label for=search-by><i class="fas fa-search"></i></label>
<input data-search-input id=search-by type=search placeholder=Search...>
<span data-search-clear><i class="fas fa-times"></i></span></div><script type=text/javascript src=../../js/lunr.min.js?1765337914></script><script type=text/javascript src=../../js/auto-complete.js?1765337914></script><script type=text/javascript>var baseurl="https://github.com/nhutruong47/report/"</script><script type=text/javascript src=../../js/search.js?1765337914></script></div><div class=highlightable><ul class=topics><li data-nav-id=/1-worklog/ title=Worklog class=dd-item><a href=../../1-worklog/><b>1. </b>Worklog
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/1-worklog/1.1-week1/ title="Week 1 Worklog" class=dd-item><a href=../../1-worklog/1.1-week1/><b>1.1. </b>Week 1 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.2-week2/ title="Week 2 Worklog" class=dd-item><a href=../../1-worklog/1.2-week2/><b>1.2. </b>Week 2 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.3-week3/ title="Week 3 Worklog" class=dd-item><a href=../../1-worklog/1.3-week3/><b>1.3. </b>Week 3 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.4-week4/ title="Week 4 Worklog" class=dd-item><a href=../../1-worklog/1.4-week4/><b>1.4. </b>Week 4 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.5-week5/ title="Week 5 Worklog" class=dd-item><a href=../../1-worklog/1.5-week5/><b>1.5. </b>Week 5 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.6-week6/ title="Week 6 Worklog" class=dd-item><a href=../../1-worklog/1.6-week6/><b>1.6. </b>Week 6 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.7-week7/ title="Week 7 Worklog" class=dd-item><a href=../../1-worklog/1.7-week7/><b>1.7. </b>Week 7 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.9-week9/ title="Week 9 Worklog" class=dd-item><a href=../../1-worklog/1.9-week9/><b>1.9. </b>Week 9 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.8-week8/ title="Week 8 Worklog" class=dd-item><a href=../../1-worklog/1.8-week8/><b>1.8. </b>Week 8 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.10-week10/ title="Week 10 Worklog" class=dd-item><a href=../../1-worklog/1.10-week10/><b>1.10. </b>Week 10 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.11-week11/ title="Week 11 Worklog" class=dd-item><a href=../../1-worklog/1.11-week11/><b>1.11. </b>Week 11 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.12-week12/ title="Week 12 Worklog" class=dd-item><a href=../../1-worklog/1.12-week12/><b>1.12. </b>Week 12 Worklog
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/2-proposal/ title=Proposal class=dd-item><a href=../../2-proposal/><b>2. </b>Proposal
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/3-blogstranslated/ title="Translated Blogs" class="dd-item
parent"><a href=../../3-blogstranslated/><b>3. </b>Translated Blogs
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/3-blogstranslated/3.1-blog1/ title="Blog 1" class="dd-item
active"><a href=../../3-blogstranslated/3.1-blog1/><b>3.1. </b>Blog 1
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/3-blogstranslated/3.2-blog2/ title="Blog 2" class=dd-item><a href=../../3-blogstranslated/3.2-blog2/><b>3.2. </b>Blog 2
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/3-blogstranslated/3.3-blog3/ title="Blog 3" class=dd-item><a href=../../3-blogstranslated/3.3-blog3/><b>3.3. </b>Blog 3
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/4-eventparticipated/ title="Events Participated" class=dd-item><a href=../../4-eventparticipated/><b>4. </b>Events Participated
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/4-eventparticipated/4.1-event1/ title="Event 1" class=dd-item><a href=../../4-eventparticipated/4.1-event1/><b>4.1. </b>Event 1
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/4-eventparticipated/4.2-event2/ title="Event 2" class=dd-item><a href=../../4-eventparticipated/4.2-event2/><b>4.2. </b>Event 2
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/4-eventparticipated/4.3-event3/ title="Event 3" class=dd-item><a href=../../4-eventparticipated/4.3-event3/><b>4.3 </b>Event 3
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/5-workshop/ title=Workshop class=dd-item><a href=../../5-workshop/><b>5. </b>Workshop
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/5-workshop/5.1-workshop-overview/ title=Introduction class=dd-item><a href=../../5-workshop/5.1-workshop-overview/><b>5.1. </b>Introduction
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.2-prerequiste/ title=Prerequiste class=dd-item><a href=../../5-workshop/5.2-prerequiste/><b>5.2. </b>Prerequiste
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.3-s3-vpc/ title="Access S3 from VPC" class=dd-item><a href=../../5-workshop/5.3-s3-vpc/><b>5.3. </b>Access S3 from VPC
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/ title="Create a gateway endpoint" class=dd-item><a href=../../5-workshop/5.3-s3-vpc/5.3.1-create-gwe/><b>5.3.1 </b>Create a gateway endpoint
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/ title="Test the Gateway Endpoint" class=dd-item><a href=../../5-workshop/5.3-s3-vpc/5.3.2-test-gwe/><b>5.3.2 </b>Test the Gateway Endpoint
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/5-workshop/5.4-s3-onprem/ title="Access S3 from on-premises" class=dd-item><a href=../../5-workshop/5.4-s3-onprem/><b>5.4. </b>Access S3 from on-premises
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/5-workshop/5.4-s3-onprem/5.4.1-prepare/ title="Prepare the environment" class=dd-item><a href=../../5-workshop/5.4-s3-onprem/5.4.1-prepare/><b>5.4.1 </b>Prepare the environment
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/ title="Create an S3 Interface endpoint" class=dd-item><a href=../../5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/><b>5.4.2 </b>Create an S3 Interface endpoint
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/ title="Test the Interface Endpoint" class=dd-item><a href=../../5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/><b>5.4.3 </b>Test the Interface Endpoint
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/ title="On-premises DNS Simulation" class=dd-item><a href=../../5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/><b>5.4.4 </b>On-premises DNS Simulation
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/5-workshop/5.5-policy/ title="VPC Endpoint Policies" class=dd-item><a href=../../5-workshop/5.5-policy/><b>5.5. </b>VPC Endpoint Policies
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/5-workshop/5.6-cleanup/ title="Clean up" class=dd-item><a href=../../5-workshop/5.6-cleanup/><b>5.6. </b>Clean up
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/6-self-evaluation/ title=Self-evaluation class=dd-item><a href=../../6-self-evaluation/><b>6. </b>Self-evaluation
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/7-feedback/ title="Share and Contribute Feedback" class=dd-item><a href=../../7-feedback/><b>7. </b>Share and Contribute Feedback
<i class="fas fa-check read-icon"></i></a></li></ul><section id=shortcuts><h3>More</h3><ul><li><a class=padding href=https://www.facebook.com/groups/awsstudygroupfcj/><i class='fab fa-facebook'></i> AWS Study Group</a></li></ul></section><section id=prefooter><hr><ul><li><a class=padding><i class="fas fa-language fa-fw"></i><div class=select-style><select id=select-language onchange="location=this.value"><option id=en value=https://github.com/nhutruong47/report/3-blogstranslated/3.1-blog1/ selected>English</option><option id=vi value=https://github.com/nhutruong47/report/vi/3-blogstranslated/3.1-blog1/>Tiếng Việt</option></select><svg id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" width="255" height="255" viewBox="0 0 255 255" style="enable-background:new 0 0 255 255"><g><g id="arrow-drop-down"><polygon points="0,63.75 127.5,191.25 255,63.75"/></g></g></svg></div></a></li><li><a class=padding href=# data-clear-history-toggle><i class="fas fa-history fa-fw"></i> Clear History</a></li></ul></section><section id=footer><left><b>Workshop</b><br><img src="https://hitwebcounter.com/counter/counter.php?page=7920860&style=0038&nbdigits=9&type=page&initCount=0" title=Migrate alt="web counter" border=0></a><br><b><a href=https://cloudjourney.awsstudygroup.com/>Cloud Journey</a></b><br><img src="https://hitwebcounter.com/counter/counter.php?page=7830807&style=0038&nbdigits=9&type=page&initCount=0" title="Total CLoud Journey" alt="web counter" border=0>
</left><left><br><br><b>Last Updated</b><br><i><span id=lastUpdated style=color:orange></span>
</i><script>const today=new Date,formattedDate=today.toLocaleDateString("en-GB");document.getElementById("lastUpdated").textContent=formattedDate</script></left><left><br><br><b>Team</b><br><i><a href=https://www.facebook.com/groups/660548818043427 style=color:orange>First Cloud Journey</a><br></i></left><script async defer src=https://buttons.github.io/buttons.js></script></section></div></nav><section id=body><div id=overlay></div><div class="padding highlightable"><div><div id=top-bar><div id=breadcrumbs itemscope itemtype=http://data-vocabulary.org/Breadcrumb><span id=sidebar-toggle-span><a href=# id=sidebar-toggle data-sidebar-toggle><i class="fas fa-bars"></i>
</a></span><span id=toc-menu><i class="fas fa-list-alt"></i></span>
<span class=links><a href=../../>Internship Report</a> > <a href=../../3-blogstranslated/>Translated Blogs</a> > Blog 1</span></div><div class=progress><div class=wrapper><nav id=TableOfContents><ul><li><a href=#solution-overview>Solution overview</a></li><li><a href=#prerequisites>Prerequisites</a></li><li><a href=#adding-emr-serverless-to-compute>Adding EMR Serverless to compute</a></li><li><a href=#developing-monitoring-and-debugging-spark-applications-in-jupyter-notebooks>Developing, monitoring, and debugging Spark applications in Jupyter notebooks</a><ul><li><a href=#implementation-steps>Implementation steps:</a></li></ul></li><li><a href=#monitoring-jobs-in-spark-ui>Monitoring jobs in Spark UI</a></li><li><a href=#cleanup>Cleanup</a></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#appendix>Appendix</a><ul><li><a href=#running-scheduled-workloads>Running scheduled workloads</a></li></ul></li><li><a href=#blog1images3-blogbdb-5127-airflow-sparkui-17jpeg><img src=../../images/3-Blog/BDB-5127-Airflow-SparkUI-17.jpeg alt=blog1></a></li><li><a href=#about-the-authors>About the authors</a></li></ul></nav></div></div></div></div><div id=head-tags></div><div id=body-inner><h1>Blog 1</h1><h1 id=develop-and-monitor-a-spark-application-using-existing-data-in-amazon-s3-with-amazon-sagemaker-unified-studio>Develop and monitor a Spark application using existing data in Amazon S3 with Amazon SageMaker Unified Studio</h1><p>by Amit Maindola and Abhilash Nagilla | on July 9, 2025 | in <a href=https://aws.amazon.com/blogs/big-data/category/analytics/amazon-sagemaker-data-ai-governance/>Amazon SageMaker Data & AI Governance</a>, <a href=https://aws.amazon.com/blogs/big-data/category/analytics/amazon-sagemaker-lakehouse/>Amazon SageMaker Lakehouse</a>, <a href=https://aws.amazon.com/blogs/big-data/category/analytics/amazon-sagemaker-unified-studio/>Amazon SageMaker Unified Studio</a>, <a href=https://aws.amazon.com/blogs/big-data/category/analytics/>Analytics</a>, <a href=https://aws.amazon.com/blogs/big-data/category/post-types/technical-how-to/>Technical How-to</a></p><hr><p>Organizations face significant challenges managing their big data analytics workloads. Data teams struggle with fragmented development environments, complex resource management, inconsistent monitoring, and cumbersome manual scheduling processes. These issues lead to lengthy development cycles, inefficient resource utilization, reactive troubleshooting, and difficult-to-maintain data pipelines. These challenges are especially critical for enterprises processing terabytes of data daily for business intelligence (BI), reporting, and machine learning (ML). Such organizations need unified solutions that streamline their entire analytics workflow.</p><p>The next generation of <a href=https://aws.amazon.com/sagemaker/>Amazon SageMaker</a> combined with <a href=https://aws.amazon.com/emr/>Amazon EMR</a> in <a href=https://aws.amazon.com/sagemaker/unified-studio/>Amazon SageMaker Unified Studio</a> addresses these pain points through an integrated development environment (IDE) where data workers can develop, test, and refine Spark applications in a consistent environment. <a href=https://aws.amazon.com/emr/serverless/>Amazon EMR Serverless</a> reduces the cluster management burden by dynamically provisioning resources based on workload requirements, and integrated monitoring tools help teams quickly identify performance bottlenecks.</p><p>Integration with <a href=https://airflow.apache.org/>Apache Airflow</a> through <a href=https://aws.amazon.com/managed-workflows-for-apache-airflow/>Amazon Managed Workflows for Apache Airflow</a> (Amazon MWAA) provides robust scheduling capabilities, and the pay-only-for-resources-used model delivers significant cost savings.</p><p>In this post, we demonstrate how to develop and monitor a Spark application using existing data in <a href=http://aws.amazon.com/s3>Amazon Simple Storage Service</a> (Amazon S3) with SageMaker Unified Studio.</p><hr><h2 id=solution-overview>Solution overview</h2><p>This solution uses SageMaker Unified Studio to execute and monitor Spark applications, highlighting integration capabilities. We demonstrate the following key steps:</p><ol><li>Create an EMR Serverless compute environment for interactive applications using SageMaker Unified Studio</li><li>Create and configure Spark applications</li><li>Use <a href=https://www.tpc.org/tpcds/>TPC-DS</a> data to build and run Spark applications using <a href=https://jupyter.org/>Jupyter</a> notebooks in SageMaker Unified Studio</li><li>Monitor application performance and schedule periodic runs with Amazon MWAA integration</li><li>Analyze results in SageMaker Unified Studio to optimize workflows</li></ol><hr><h2 id=prerequisites>Prerequisites</h2><p>To follow this tutorial, you need the following requirements:</p><ul><li><strong>An AWS account</strong> – If you don&rsquo;t have an account yet, you can <a href=https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/>create one</a></li><li><strong>A SageMaker Unified Studio domain</strong> – For instructions, see <a href=https://docs.aws.amazon.com/sagemaker-unified-studio/latest/adminguide/create-domain-sagemaker-unified-studio-quick.html>Create an Amazon SageMaker Unified Studio domain – quick setup</a></li><li><strong>A demo project</strong> – Create a demo project in your SageMaker Unified Studio domain. For instructions, see <a href=https://docs.aws.amazon.com/sagemaker-unified-studio/latest/userguide/getting-started-create-a-project.html>Create a project</a>. For this example, we choose a profile with <strong>All capabilities</strong> in the project configuration section</li></ul><hr><h2 id=adding-emr-serverless-to-compute>Adding EMR Serverless to compute</h2><p>Complete the following steps to create an EMR Serverless compute environment for building Spark applications:</p><ol><li>In SageMaker Unified Studio, open the project you created as a prerequisite and choose <strong>Compute</strong></li><li>Choose <strong>Data processing</strong>, then choose <strong>Add compute</strong></li><li>Choose <strong>Create new compute resources</strong>, then choose <strong>Next</strong>
<img alt=blog1 src=../../images/3-Blog/BDB-5127-EMR-Add-Compute-1-New.png></li><li>Choose <strong>EMR Serverless</strong>, then choose <strong>Next</strong>
<img alt=blog1 src=../../images/3-Blog/BDB-5127-EMR-Serverless-2-New.png></li><li>Enter a name for <strong>Compute name</strong></li><li>For <strong>Release label</strong>, choose <strong>emr-7.5.0</strong></li><li>For <strong>Permission mode</strong>, choose <strong>Compatibility</strong></li><li>Choose <strong>Add compute</strong></li></ol><p>The EMR Serverless application initialization process takes a few minutes. After creation is complete, you can view the compute in SageMaker Unified Studio.
<img alt=blog1 src=../../images/3-Blog/BDB-5127-Compute-3-1.png></p><p>The above steps illustrate how you set up an Amazon EMR Serverless application in SageMaker Unified Studio to run interactive PySpark workloads. In the following steps, we will build and monitor the Spark application in the interactive JupyterLab workspace.</p><hr><h2 id=developing-monitoring-and-debugging-spark-applications-in-jupyter-notebooks>Developing, monitoring, and debugging Spark applications in Jupyter notebooks</h2><p>In this section, we build a Spark application using the TPC-DS dataset in SageMaker Unified Studio. With <a href=https://aws.amazon.com/sagemaker/data-processing/>Amazon SageMaker Data Processing</a>, you can focus on transforming and analyzing data without having to manage compute capacity or open-source applications, saving time and reducing costs.</p><p>SageMaker Data Processing provides a unified development experience from Amazon EMR, <a href=https://aws.amazon.com/glue>AWS Glue</a>, <a href=http://aws.amazon.com/redshift>Amazon Redshift</a>, <a href=http://aws.amazon.com/athena>Amazon Athena</a>, and Amazon MWAA in the same notebook and query interface. You can automatically provision capacity on <a href=http://aws.amazon.com/ec2>Amazon Elastic Compute Cloud</a> (Amazon EC2) or EMR Serverless. Autoscaling rules manage changing compute demands to optimize performance and execution time.</p><h3 id=implementation-steps>Implementation steps:</h3><ol><li>After completing the previous preparation steps, go to SageMaker Studio and open your project</li><li>Choose <strong>Build</strong>, then choose <strong>JupyterLab</strong>
The notebook takes about 30 seconds to initialize and connect to the workspace</li><li>Under the <strong>Notebook</strong> category, choose <strong>Python 3 (ipykernel)</strong></li><li>In the first cell, next to <strong>Local Python</strong>, select the dropdown menu and choose <strong>PySpark</strong></li><li>Choose the dropdown menu next to <strong>Project.Spark</strong> and select the <strong>EMR-S Compute</strong> compute</li><li>Run the following code to develop your Spark application. This example reads a 3 TB TPC-DS dataset in Parquet format from a public S3 bucket:</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>parquet(<span style=color:#e6db74>&#34;s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/store/&#34;</span>)<span style=color:#f92672>.</span>createOrReplaceTempView(<span style=color:#e6db74>&#34;store&#34;</span>)
</span></span></code></pre></div><p>When the Spark session starts and execution logs begin to appear, you can explore the Spark UI and driver logs to debug and troubleshoot your Spark program.
<img alt=blog1 src=../../images/3-Blog/BDB-5127-UI-Driver-4.png>
The following screenshot shows an example of the Spark user interface.
<img alt=blog1 src=../../images/3-Blog/BDB-5127-SparkUI-5.png>
The following screenshot shows an example of driver logs.
<img alt=blog1 src=../../images/3-Blog/BDB-5127-Spark-Driver-6.png>
The following screenshot shows the Executors tab, providing access to driver and executor logs.
<img alt=blog1 src=../../images/3-Blog/BDB-5127-Executors-7.png></p><ol start=7><li>Use the following code to read additional TPC-DS datasets. You can create temporary views and use the Spark UI to view the files being read. Refer to the appendix at the end of this article for details on how to use TPC-DS datasets in your buckets.</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>parquet(<span style=color:#e6db74>&#34;s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/item/&#34;</span>)<span style=color:#f92672>.</span>createOrReplaceTempView(<span style=color:#e6db74>&#34;item&#34;</span>)
</span></span><span style=display:flex><span>spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>parquet(<span style=color:#e6db74>&#34;s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/store_sales/&#34;</span>)<span style=color:#f92672>.</span>createOrReplaceTempView(<span style=color:#e6db74>&#34;store_sales&#34;</span>)
</span></span><span style=display:flex><span>spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>parquet(<span style=color:#e6db74>&#34;s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/date_dim/&#34;</span>)<span style=color:#f92672>.</span>createOrReplaceTempView(<span style=color:#e6db74>&#34;date_dim&#34;</span>)
</span></span><span style=display:flex><span>spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>parquet(<span style=color:#e6db74>&#34;s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/customer/&#34;</span>)<span style=color:#f92672>.</span>createOrReplaceTempView(<span style=color:#e6db74>&#34;customer&#34;</span>)
</span></span><span style=display:flex><span>spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>parquet(<span style=color:#e6db74>&#34;s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/catalog_sales/&#34;</span>)<span style=color:#f92672>.</span>createOrReplaceTempView(<span style=color:#e6db74>&#34;catalog_sales&#34;</span>)
</span></span><span style=display:flex><span>spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>parquet(<span style=color:#e6db74>&#34;s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/web_sales/&#34;</span>)<span style=color:#f92672>.</span>createOrReplaceTempView(<span style=color:#e6db74>&#34;web_sales&#34;</span>)
</span></span></code></pre></div><p>In each notebook cell, you can open Spark Job Progress to view the stages of jobs sent to EMR Serverless for a specific cell. You can see the completion time for each stage. If errors occur, you can check the logs, making troubleshooting more seamless.
<img alt=blog1 src=../../images/3-Blog/BDB-5127-Spark-Job-Progress-8.png>
Since the files are partitioned based on date key columns, you can observe that Spark runs tasks in parallel to read the data.
<img alt=blog1 src=../../images/3-Blog/BDB-5127-SparkJobs-9.png>
8. Next, get counts by date keys on data partitioned by time key using the following code:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>select count(<span style=color:#ae81ff>1</span>), ss_sold_date_sk <span style=color:#f92672>from</span> store_sales group by ss_sold_date_sk order by ss_sold_date_sk
</span></span></code></pre></div><p><img alt=blog1 src=../../images/3-Blog/BDB-5127-Notebook-Block-10.png></p><h2 id=monitoring-jobs-in-spark-ui>Monitoring jobs in Spark UI</h2><p>In the <strong>Jobs</strong> tab of the Spark UI, you can view a list of completed or running jobs, with the following information:</p><ul><li>Action that triggered the job</li><li>Execution time (e.g., 41 seconds, but times will vary)</li><li>Number of stages and tasks — in this example, 2 stages and 3,428 tasks</li></ul><p>You can select a job to see more details, especially about the stages. Our job has two stages; a new stage is created each time there is a shuffle. We have one stage to read data from each initial dataset, and one stage for aggregation.
<img alt=blog1 src=../../images/3-Blog/BDB-5127-JobRun-11.gif>
In the next example, we run some TPC-DS SQL queries used for performance evaluation and benchmarking:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>with</span> frequent_ss_items <span style=color:#66d9ef>as</span>
</span></span><span style=display:flex><span> (<span style=color:#66d9ef>select</span> substr(i_item_desc,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>30</span>) itemdesc,i_item_sk item_sk,d_date solddate,<span style=color:#66d9ef>count</span>(<span style=color:#f92672>*</span>) cnt
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>from</span> store_sales, date_dim, item
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>where</span> ss_sold_date_sk <span style=color:#f92672>=</span> d_date_sk
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>and</span> ss_item_sk <span style=color:#f92672>=</span> i_item_sk
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>and</span> d_year <span style=color:#66d9ef>in</span> (<span style=color:#ae81ff>2000</span>, <span style=color:#ae81ff>2000</span><span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2000</span><span style=color:#f92672>+</span><span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>2000</span><span style=color:#f92672>+</span><span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>group</span> <span style=color:#66d9ef>by</span> substr(i_item_desc,<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>30</span>),i_item_sk,d_date
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>having</span> <span style=color:#66d9ef>count</span>(<span style=color:#f92672>*</span>) <span style=color:#f92672>&gt;</span><span style=color:#ae81ff>4</span>),
</span></span><span style=display:flex><span> max_store_sales <span style=color:#66d9ef>as</span>
</span></span><span style=display:flex><span> (<span style=color:#66d9ef>select</span> <span style=color:#66d9ef>max</span>(csales) tpcds_cmax
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>from</span> (<span style=color:#66d9ef>select</span> c_customer_sk,<span style=color:#66d9ef>sum</span>(ss_quantity<span style=color:#f92672>*</span>ss_sales_price) csales
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>from</span> store_sales, customer, date_dim
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>where</span> ss_customer_sk <span style=color:#f92672>=</span> c_customer_sk
</span></span><span style=display:flex><span>         <span style=color:#66d9ef>and</span> ss_sold_date_sk <span style=color:#f92672>=</span> d_date_sk
</span></span><span style=display:flex><span>         <span style=color:#66d9ef>and</span> d_year <span style=color:#66d9ef>in</span> (<span style=color:#ae81ff>2000</span>, <span style=color:#ae81ff>2000</span><span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2000</span><span style=color:#f92672>+</span><span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>2000</span><span style=color:#f92672>+</span><span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>group</span> <span style=color:#66d9ef>by</span> c_customer_sk) x),
</span></span><span style=display:flex><span> best_ss_customer <span style=color:#66d9ef>as</span>
</span></span><span style=display:flex><span> (<span style=color:#66d9ef>select</span> c_customer_sk,<span style=color:#66d9ef>sum</span>(ss_quantity<span style=color:#f92672>*</span>ss_sales_price) ssales
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>from</span> store_sales, customer
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>where</span> ss_customer_sk <span style=color:#f92672>=</span> c_customer_sk
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>group</span> <span style=color:#66d9ef>by</span> c_customer_sk
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>having</span> <span style=color:#66d9ef>sum</span>(ss_quantity<span style=color:#f92672>*</span>ss_sales_price) <span style=color:#f92672>&gt;</span> (<span style=color:#ae81ff>95</span><span style=color:#f92672>/</span><span style=color:#ae81ff>100</span>.<span style=color:#ae81ff>0</span>) <span style=color:#f92672>*</span>
</span></span><span style=display:flex><span>    (<span style=color:#66d9ef>select</span> <span style=color:#f92672>*</span> <span style=color:#66d9ef>from</span> max_store_sales))
</span></span><span style=display:flex><span> <span style=color:#66d9ef>select</span> <span style=color:#66d9ef>sum</span>(sales)
</span></span><span style=display:flex><span> <span style=color:#66d9ef>from</span> (<span style=color:#66d9ef>select</span> cs_quantity<span style=color:#f92672>*</span>cs_list_price sales
</span></span><span style=display:flex><span>       <span style=color:#66d9ef>from</span> catalog_sales, date_dim
</span></span><span style=display:flex><span>       <span style=color:#66d9ef>where</span> d_year <span style=color:#f92672>=</span> <span style=color:#ae81ff>2000</span>
</span></span><span style=display:flex><span>         <span style=color:#66d9ef>and</span> d_moy <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>         <span style=color:#66d9ef>and</span> cs_sold_date_sk <span style=color:#f92672>=</span> d_date_sk
</span></span><span style=display:flex><span>         <span style=color:#66d9ef>and</span> cs_item_sk <span style=color:#66d9ef>in</span> (<span style=color:#66d9ef>select</span> item_sk <span style=color:#66d9ef>from</span> frequent_ss_items)
</span></span><span style=display:flex><span>         <span style=color:#66d9ef>and</span> cs_bill_customer_sk <span style=color:#66d9ef>in</span> (<span style=color:#66d9ef>select</span> c_customer_sk <span style=color:#66d9ef>from</span> best_ss_customer)
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>union</span> <span style=color:#66d9ef>all</span>
</span></span><span style=display:flex><span>      (<span style=color:#66d9ef>select</span> ws_quantity<span style=color:#f92672>*</span>ws_list_price sales
</span></span><span style=display:flex><span>       <span style=color:#66d9ef>from</span> web_sales, date_dim
</span></span><span style=display:flex><span>       <span style=color:#66d9ef>where</span> d_year <span style=color:#f92672>=</span> <span style=color:#ae81ff>2000</span>
</span></span><span style=display:flex><span>         <span style=color:#66d9ef>and</span> d_moy <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>         <span style=color:#66d9ef>and</span> ws_sold_date_sk <span style=color:#f92672>=</span> d_date_sk
</span></span><span style=display:flex><span>         <span style=color:#66d9ef>and</span> ws_item_sk <span style=color:#66d9ef>in</span> (<span style=color:#66d9ef>select</span> item_sk <span style=color:#66d9ef>from</span> frequent_ss_items)
</span></span><span style=display:flex><span>         <span style=color:#66d9ef>and</span> ws_bill_customer_sk <span style=color:#66d9ef>in</span> (<span style=color:#66d9ef>select</span> c_customer_sk <span style=color:#66d9ef>from</span> best_ss_customer))) x
</span></span></code></pre></div><p>You can monitor Spark jobs in SageMaker Unified Studio in two ways. Jupyter notebooks provide basic monitoring, showing real-time job status and execution progress. For more detailed analysis, use the Spark UI. You can check specific stages, tasks, and execution plans. The Spark UI is particularly useful for troubleshooting performance issues and optimizing queries. You can track the expected number of stages, running tasks, and detailed duration of each task. This comprehensive view helps you understand resource usage and track job progress at a detailed level.</p><p><img alt=blog1 src=../../images/3-Blog/BDB-5127-Spark-TPCDS-12.gif>
In this section, we explained how you can use EMR Serverless compute in SageMaker Unified Studio to build interactive Spark applications. Through the Spark UI, interactive applications provide detailed task-level status, I/O and shuffle information, as well as links to corresponding task logs directly from the notebook, enabling a seamless debugging experience.</p><hr><h2 id=cleanup>Cleanup</h2><p>To avoid ongoing charges in your AWS account, delete the resources you created in this tutorial:</p><ol><li>Delete connections</li><li>Delete EMR jobs</li><li>Delete EMR output S3 buckets</li><li>Delete Amazon MWAA resources such as workflows and environments</li></ol><hr><h2 id=conclusion>Conclusion</h2><p>In this post, we demonstrated how the next generation of SageMaker, combined with EMR Serverless, provides a powerful solution for developing, monitoring, and scheduling Spark applications using data in Amazon S3. The integrated experience significantly reduces complexity by providing a unified development environment, automatic resource management, and comprehensive monitoring capabilities through the Spark UI, while maintaining cost efficiency with the pay-as-you-use model. For enterprises, this means faster time to insights, improved team collaboration, and reduced operational overhead, allowing data teams to focus on analytics rather than infrastructure management.</p><p>To get started, explore the <a href=https://docs.aws.amazon.com/sagemaker-unified-studio/latest/userguide/what-is-sagemaker-unified-studio.html>Amazon SageMaker Unified Studio User Guide</a>, set up a project in your AWS environment, and discover how this solution can transform your organization&rsquo;s data analytics capabilities.</p><hr><h2 id=appendix>Appendix</h2><p>In the following sections, we discuss how to run scheduled workloads and provide details about the TPC-DS dataset for building Spark applications using EMR Serverless.</p><h3 id=running-scheduled-workloads>Running scheduled workloads</h3><p>In this section, we deploy JupyterLab notebooks and create workflows using Amazon MWAA. You can use workflows to orchestrate notebooks, querybooks, and many other things in your project repository. With workflows, you can define a set of tasks organized as a directed acyclic graph (DAG) that can run on a schedule you define. Implementation steps:</p><ol><li><p>In SageMaker Unified Studio, choose <strong>Build</strong>, and under <strong>Orchestration</strong>, choose <strong>Workflows</strong>
<img alt=blog1 src=../../images/3-Blog/BDB-5127-Workflow-13.png></p></li><li><p>Choose <strong>Create Workflow in Editor</strong>
You will be directed to the JupyterLab notebook with a new DAG named <code>untitled.py</code> created in the <code>/src/workflows/dag</code> folder</p></li><li><p>Rename this notebook to <code>tpcds_data_queries.py</code></p></li><li><p>You can reuse the existing template with the following updates:</p></li></ol><p>a. Update line 17 with the schedule you want your code to run on
b. Update line 26 with your NOTEBOOK_PATH. This path should be in <code>src/&lt;notebook_name>.ipynb</code>. Note that the dag_id name is auto-generated; you can name it as per your requirement.
<img alt=blog1 src=../../images/3-Blog/BDB-5127-Spark-TPCDS-DagNotebk-14.jpg>
5. Choose <strong>File</strong> and <strong>Save notebook</strong>
To test, you can trigger a manual workload run
6. In SageMaker Unified Studio, choose <strong>Build</strong>, then under <strong>Orchestration</strong>, choose <strong>Workflows</strong>.
7. Choose your workflow, then choose <strong>Run</strong>.
You can monitor job success on the Runs tab.
<img alt=blog1 src=../../images/3-Blog/BDB-5127-AirflowRun-15.png>
To debug notebook jobs by accessing the Spark UI in the Airflow job console, you must use <a href=https://docs.aws.amazon.com/emr/latest/EMR-Serverless-UserGuide/using-airflow.html>EMR Serverless Airflow Operators</a> to submit jobs. The link is available on the <strong>Details</strong> tab of the query.
This option has key limitations: it is not available for Amazon EMR on EC2, and SageMaker notebook job operators do not work.</p><p>You can configure the operator to create one-time links to the application UI and Spark output logs by passing <code>enable_application_ui_links=True</code> as a parameter. After the job starts running, these links are available on the Details tab of the corresponding task. If <code>enable_application_ui_links=False</code>, the links will appear but in a grayed-out state.</p><p>Make sure you have <code>emr-serverless:GetDashboardForJobRun</code> in <a href=https://aws.amazon.com/iam/>AWS Identity and Access Management</a> (IAM) to create dashboard links.</p><p>Open the Airflow user interface for your task. The Spark user interface and history server dashboard options will appear on the Details tab, as shown in the following screenshot.</p><p><img alt=blog1 src=../../images/3-Blog/BDB-5127-AirflowUI-16.jpeg></p><p>The screenshot illustrates the Jobs tab of the Spark UI.</p><h2 id=blog1images3-blogbdb-5127-airflow-sparkui-17jpeg><img alt=blog1 src=../../images/3-Blog/BDB-5127-Airflow-SparkUI-17.jpeg></h2><h2 id=about-the-authors>About the authors</h2><div style=display:flex;align-items:flex-start;margin-bottom:30px><img src=../../images/3-Blog/Amit-Maindola.jpg alt="Amit Maindola" style=width:150px;height:150px;object-fit:cover;margin-right:20px;border-radius:8px><div><p><strong>Amit Maindola</strong> is a Senior Data Architect focused on data engineering, analytics, and AI/ML at Amazon Web Services. He helps customers in their digital transformation journey and enables them to build highly scalable, robust, and secure cloud-based analytical solutions on AWS to gain timely insights and make critical business decisions.</p></div></div><div style=display:flex;align-items:flex-start;margin-bottom:30px><img src=../../images/3-Blog/image045.jpg alt="Abhilash Nagilla" style=width:150px;height:150px;object-fit:cover;margin-right:20px;border-radius:8px><div><p><strong>Abhilash Nagilla</strong> is a senior specialist solutions architect at Amazon Web Services (AWS), supporting public sector customers on their cloud journey with a focus on AWS data and AI services. Outside of work, Abhilash enjoys learning new technologies, watching movies, and traveling to new places.</p></div></div><footer class=footline></footer></div></div><div id=navigation><a class="nav nav-prev" href=../../3-blogstranslated/ title="Translated Blogs"><i class="fa fa-chevron-left"></i></a>
<a class="nav nav-next" href=../../3-blogstranslated/3.2-blog2/ title="Blog 2" style=margin-right:0><i class="fa fa-chevron-right"></i></a></div></section><div style=left:-1000px;overflow:scroll;position:absolute;top:-1000px;border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px><div style=border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px></div></div><script src=../../js/clipboard.min.js?1765337914></script><script src=../../js/perfect-scrollbar.min.js?1765337914></script><script src=../../js/perfect-scrollbar.jquery.min.js?1765337914></script><script src=../../js/jquery.sticky.js?1765337914></script><script src=../../js/featherlight.min.js?1765337914></script><script src=../../js/highlight.pack.js?1765337914></script><script>hljs.initHighlightingOnLoad()</script><script src=../../js/modernizr.custom-3.6.0.js?1765337914></script><script src=../../js/learn.js?1765337914></script><script src=../../js/hugo-learn.js?1765337914></script><link href=../../mermaid/mermaid.css?1765337914 rel=stylesheet><script src=../../mermaid/mermaid.js?1765337914></script><script>mermaid.initialize({startOnLoad:!0})</script><script>(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,(e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date),(i=t.createElement(n),a=t.getElementsByTagName(n)[0]),i.async=1,i.src=s,a.parentNode.insertBefore(i,a)})(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-158079754-2","auto"),ga("send","pageview")</script></body></html>