[{"uri":"https://github.com/nhutruong47/report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: ‚ÄúAI/ML/GenAI on AWS‚Äù Event Objectives Provide an overview of the AI/ML/GenAI ecosystem on AWS Guide participants through the full ML lifecycle using Amazon SageMaker Explain and demo Foundation Models on Amazon Bedrock Present Prompt Engineering, RAG, and Bedrock Agents techniques Build a GenAI chatbot through a live demo Agenda-based Highlights 8:30 ‚Äì 9:00 AM | Welcome \u0026amp; Introduction Main activities:\nParticipant check-in and networking Workshop learning objectives Ice-breaker activity Overview of the AI/ML landscape in Vietnam Key takeaway: Gained a clearer view of AI/ML trends in Vietnamese enterprises and how AWS supports digital transformation with GenAI and ML.\n9:00 ‚Äì 10:30 AM | AWS AI/ML Services Overview Main content:\nIntroduction to Amazon SageMaker as an end-to-end ML platform Data preparation and labeling Model training, tuning, and deployment Integrated MLOps in SageMaker Live Demo: SageMaker Studio walkthrough Key takeaway: Understood the full ML development lifecycle on AWS, how to prepare data, monitor training/tuning/deployment, grasped real-world MLOps, and experienced SageMaker Studio and workflow hands-on.\n10:30 ‚Äì 10:45 AM | Coffee Break Short networking and discussion break with AWS experts and other participants.\n10:45 AM ‚Äì 12:00 PM | Generative AI with Amazon Bedrock Main content:\nFoundation Models: Claude, Llama, Titan ‚Äî comparison \u0026amp; selection guide Prompt Engineering: Techniques, Chain-of-Thought reasoning, Few-shot learning RAG (Retrieval-Augmented Generation): Architecture, Knowledge Base integration Bedrock Agents: Multi-step workflows, tool integrations Guardrails: Safety, content filtering Live Demo: Building a GenAI chatbot with Amazon Bedrock Key takeaway: Learned how to select the right Foundation Model, master prompt engineering (CoT, few-shot), build a complete RAG system, use Bedrock Agents for multi-step workflows, understand content safety standards and Guardrails, and follow the full GenAI chatbot creation process.\nKey Takeaways Comprehensive understanding of ML and GenAI on AWS Understood the differences and use cases for FM models (Claude/Llama/Titan) Applied Chain-of-Thought and Few-shot to improve output quality Learned to design prompts for complex pipelines Understood why RAG is needed in enterprise apps and how to connect Bedrock to Knowledge Base Learned to build agents with tool integration Hands-on experience with SageMaker Studio and end-to-end ML workflow Understood how to deploy enterprise AI chatbots using AWS standards Applying to Work Apply RAG to internal chatbots or document support systems Use SageMaker to train/fine-tune ML models Use Prompt Engineering to improve FM output quality Integrate Bedrock Agents to automate workflows Build GenAI demos for team/project Event Experience Attending ‚ÄúAI/ML/GenAI on AWS‚Äù was an extremely valuable experience, deepening my understanding of how enterprises implement AI/ML and GenAI in practice.\nHighlights Learning from AWS experts: Clear analysis of the AI/ML roadmap for Vietnamese enterprises, real-world demos of SageMaker and Bedrock. Hands-on demo sessions: Directly observed the train ‚Üí tune ‚Üí deploy process, Bedrock chatbot demo clarified the GenAI app building workflow. Networking \u0026amp; Discussions: Opportunities to discuss with AWS engineers and other participants, learn from real GenAI case studies. Lessons learned: GenAI is not just a model but a complete workflow (Prompt ‚Üí RAG ‚Üí Agents ‚Üí Guardrails), SageMaker standardizes the ML lifecycle, and model selection is crucial for efficiency and cost. Some event photos Overall, the event not only provided technical knowledge but also helped me reshape my thinking about AI/ML application, system modernization, and more effective cross-team collaboration.\n"},{"uri":"https://github.com/nhutruong47/report/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Create a gateway endpoint","tags":[],"description":"","content":" Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "},{"uri":"https://github.com/nhutruong47/report/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "},{"uri":"https://github.com/nhutruong47/report/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Prepare the environment","tags":[],"description":"","content":"To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "},{"uri":"https://github.com/nhutruong47/report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Met with AWS members and administrators - Attended an AWS event - Located teammates and formed a project group 06/09/2025 06/09/2025 2 - Module 01: Overview \u0026amp; getting started with Cloud + What is Cloud Computing? + What makes AWS unique? + How to begin your Cloud journey + AWS global infrastructure + Tools for managing AWS services + Cost optimization strategies and working patterns + Hands-on practice and further reading 10/09/2025 10/09/2025 https://000001.awsstudygroup.com/ 3 - Create a new AWS account - Configure MFA for accounts - Create Admin group and Admin user accounts - Provide account authentication support - Explore and configure the AWS Management Console - Create and manage AWS Support cases 11/09/2025 11/09/2025 https://000001.awsstudygroup.com/ 4 - Learn about AWS Budgets + Create budgets from templates + Create cost budgets + Create usage budgets + Create RI budgets + Create Savings Plans budgets + Clean up unused or redundant budgets 12/09/2025 12/09/2025 https://000007.awsstudygroup.com/ 5 - Learn about AWS Support plans - Access AWS Support services + Types of support requests + How to change support plans - Manage support requests + Create support requests + Choose appropriate severity levels 13/09/2025 13/09/2025 https://000009.awsstudygroup.com/ 6 - Module 02: AWS Virtual Private Cloud (VPC) + VPC security and multi-VPC features + Overview: VPN, Direct Connect, Load Balancers - Start with Amazon VPC and AWS Site-to-Site VPN - Firewalls in VPC: Security Groups \u0026amp; Network ACLs + VPC resource mapping - Preparation steps: + Create VPC + Create subnets + Create Internet Gateway + Create route tables + Create security groups + Enable VPC Flow Logs 14/09/2025 14/09/2025 https://www.youtube.com/watch?v=O9Ac_vGHquM\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=25 https://www.youtube.com/watch?v=BPuD1l2hEQ4\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=26 https://www.youtube.com/watch?v=CXU8D3kyxIc\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=27 https://000003.awsstudygroup.com/ üèÜ Week 1 Achievements 1. Networking \u0026amp; Collaboration\nMet AWS members and administrators; took part in introductory sessions. Attended an AWS event and participated in group activities. Formed a project team and began coordinating roles. 2. Module 01 ‚Äì AWS Cloud Fundamentals\nCovered fundamental concepts of Cloud Computing and AWS global infrastructure. Explored AWS management tools and practical cost-optimization techniques. Created and configured a new AWS account with MFA; set up Admin group and user. Practiced opening and managing Support Cases via the AWS Console. 3. AWS Budgets \u0026amp; Support\nCreated budgets (cost, usage, RI, Savings Plans) and reviewed reporting settings. Reviewed AWS Support packages and the workflow for creating support requests. Practiced selecting severity levels and managing case escalations. 4. Module 02 ‚Äì AWS VPC\nDeployed VPC resources: VPC, subnets, route tables and security groups. Enabled VPC Flow Logs and checked sample traffic logs. Studied VPN, Direct Connect, Load Balancers and network security (Security Groups, NACLs). "},{"uri":"https://github.com/nhutruong47/report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Dang Truong Huy\nPhone Number: 0913758911\nEmail: @fpt.edu.vn\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS092025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 24/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://github.com/nhutruong47/report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, I provide an overview of my worklog: I completed the internship program over 12 weeks. Each week I studied theory, completed labs, and recorded the AWS services and tasks I worked on.\nThe worklog spans about three months, so I divided the content into 12 weekly sections below to show what I did each week and the AWS knowledge and skills I gained.\nWeek 1: Understand basic AWS services, how to use the console \u0026amp; CLI.\nWeek 2: Set up Hybrid DNS with Route 53 Resolver and VPC peering\nWeek 3: Set up AWS Transit Gateway. Create Transit Gateway Attachments and Route Tables. Learn Amazon EC2 comprehensive concepts and services. Study EC2 Auto Scaling, EFS/FSx, Lightsail, and MGN\nWeek 4: Deploy AWS Backup to automate data protection. Learn AWS Storage Gateway for hybrid cloud storage. Start with Amazon S3 fundamentals and static website hosting\nWeek 5: Master AWS Storage Services and comprehensive Amazon S3 features. Learn AWS Backup and VM Import/Export strategies. Practice Storage Gateway for hybrid cloud storage solutions\nWeek 6: Understand Amazon FSx for Windows File Server architecture and use cases. Deploy and configure multi-AZ FSx file systems (SSD \u0026amp; HDD). Practice performance testing, monitoring, and optimization on FSx\nWeek 7: Understand Amazon S3 fundamentals and main use cases. Create and configure S3 buckets for static website hosting. Practice access control, CloudFront integration, and versioning Week 8: Review AWS shared responsibility model and core security/IAM services. Learn and practice AWS Security Hub with security standards. Learn how to optimize EC2 costs using AWS Lambda automation\nWeek 9: Learn how to use tags to manage AWS resources. Practice creating Resource Groups and filtering resources by tags. Manage access to EC2 using resource tags and IAM policies. Understand IAM permission boundaries and how to limit user permissions\nWeek 10: Practice encryption at rest using AWS KMS with S3 and CloudTrail/Athena. Review IAM roles, conditions, and access control patterns. Practice IAM roles for applications (EC2 -\u0026gt; S3). Learn core AWS database services: Amazon RDS, Aurora, Redshift, ElastiCache\nWeek 11: Practice building and managing relational databases using Amazon RDS. Explore database migration tools and scenarios with Lab 43. Build a simple data lake on AWS with S3, Glue, Athena, and QuickSight. Learn to create and work with Amazon DynamoDB, including backups and migration\nWeek 12: Practice building and inspecting cost-related data in AWS. Get familiar with different ways to interact with AWS (CloudShell, Console, SDK). Use AWS Glue DataBrew and Cloud9 to prepare and transform data. Build an end-to-end analytics pipeline with Glue, EMR, Athena, Kinesis Data Analytics, QuickSight, Lambda, and Redshift\n"},{"uri":"https://github.com/nhutruong47/report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Develop and monitor a Spark application using existing data in Amazon S3 with Amazon SageMaker Unified Studio by Amit Maindola and Abhilash Nagilla | on July 9, 2025 | in Amazon SageMaker Data \u0026amp; AI Governance, Amazon SageMaker Lakehouse, Amazon SageMaker Unified Studio, Analytics, Technical How-to\nOrganizations face significant challenges managing their big data analytics workloads. Data teams struggle with fragmented development environments, complex resource management, inconsistent monitoring, and cumbersome manual scheduling processes. These issues lead to lengthy development cycles, inefficient resource utilization, reactive troubleshooting, and difficult-to-maintain data pipelines. These challenges are especially critical for enterprises processing terabytes of data daily for business intelligence (BI), reporting, and machine learning (ML). Such organizations need unified solutions that streamline their entire analytics workflow.\nThe next generation of Amazon SageMaker combined with Amazon EMR in Amazon SageMaker Unified Studio addresses these pain points through an integrated development environment (IDE) where data workers can develop, test, and refine Spark applications in a consistent environment. Amazon EMR Serverless reduces the cluster management burden by dynamically provisioning resources based on workload requirements, and integrated monitoring tools help teams quickly identify performance bottlenecks.\nIntegration with Apache Airflow through Amazon Managed Workflows for Apache Airflow (Amazon MWAA) provides robust scheduling capabilities, and the pay-only-for-resources-used model delivers significant cost savings.\nIn this post, we demonstrate how to develop and monitor a Spark application using existing data in Amazon Simple Storage Service (Amazon S3) with SageMaker Unified Studio.\nSolution overview This solution uses SageMaker Unified Studio to execute and monitor Spark applications, highlighting integration capabilities. We demonstrate the following key steps:\nCreate an EMR Serverless compute environment for interactive applications using SageMaker Unified Studio Create and configure Spark applications Use TPC-DS data to build and run Spark applications using Jupyter notebooks in SageMaker Unified Studio Monitor application performance and schedule periodic runs with Amazon MWAA integration Analyze results in SageMaker Unified Studio to optimize workflows Prerequisites To follow this tutorial, you need the following requirements:\nAn AWS account ‚Äì If you don\u0026rsquo;t have an account yet, you can create one A SageMaker Unified Studio domain ‚Äì For instructions, see Create an Amazon SageMaker Unified Studio domain ‚Äì quick setup A demo project ‚Äì Create a demo project in your SageMaker Unified Studio domain. For instructions, see Create a project. For this example, we choose a profile with All capabilities in the project configuration section Adding EMR Serverless to compute Complete the following steps to create an EMR Serverless compute environment for building Spark applications:\nIn SageMaker Unified Studio, open the project you created as a prerequisite and choose Compute Choose Data processing, then choose Add compute Choose Create new compute resources, then choose Next Choose EMR Serverless, then choose Next Enter a name for Compute name For Release label, choose emr-7.5.0 For Permission mode, choose Compatibility Choose Add compute The EMR Serverless application initialization process takes a few minutes. After creation is complete, you can view the compute in SageMaker Unified Studio. The above steps illustrate how you set up an Amazon EMR Serverless application in SageMaker Unified Studio to run interactive PySpark workloads. In the following steps, we will build and monitor the Spark application in the interactive JupyterLab workspace.\nDeveloping, monitoring, and debugging Spark applications in Jupyter notebooks In this section, we build a Spark application using the TPC-DS dataset in SageMaker Unified Studio. With Amazon SageMaker Data Processing, you can focus on transforming and analyzing data without having to manage compute capacity or open-source applications, saving time and reducing costs.\nSageMaker Data Processing provides a unified development experience from Amazon EMR, AWS Glue, Amazon Redshift, Amazon Athena, and Amazon MWAA in the same notebook and query interface. You can automatically provision capacity on Amazon Elastic Compute Cloud (Amazon EC2) or EMR Serverless. Autoscaling rules manage changing compute demands to optimize performance and execution time.\nImplementation steps: After completing the previous preparation steps, go to SageMaker Studio and open your project Choose Build, then choose JupyterLab The notebook takes about 30 seconds to initialize and connect to the workspace Under the Notebook category, choose Python 3 (ipykernel) In the first cell, next to Local Python, select the dropdown menu and choose PySpark Choose the dropdown menu next to Project.Spark and select the EMR-S Compute compute Run the following code to develop your Spark application. This example reads a 3 TB TPC-DS dataset in Parquet format from a public S3 bucket: spark.read.parquet(\u0026#34;s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/store/\u0026#34;).createOrReplaceTempView(\u0026#34;store\u0026#34;) When the Spark session starts and execution logs begin to appear, you can explore the Spark UI and driver logs to debug and troubleshoot your Spark program. The following screenshot shows an example of the Spark user interface. The following screenshot shows an example of driver logs. The following screenshot shows the Executors tab, providing access to driver and executor logs. Use the following code to read additional TPC-DS datasets. You can create temporary views and use the Spark UI to view the files being read. Refer to the appendix at the end of this article for details on how to use TPC-DS datasets in your buckets. spark.read.parquet(\u0026#34;s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/item/\u0026#34;).createOrReplaceTempView(\u0026#34;item\u0026#34;) spark.read.parquet(\u0026#34;s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/store_sales/\u0026#34;).createOrReplaceTempView(\u0026#34;store_sales\u0026#34;) spark.read.parquet(\u0026#34;s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/date_dim/\u0026#34;).createOrReplaceTempView(\u0026#34;date_dim\u0026#34;) spark.read.parquet(\u0026#34;s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/customer/\u0026#34;).createOrReplaceTempView(\u0026#34;customer\u0026#34;) spark.read.parquet(\u0026#34;s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/catalog_sales/\u0026#34;).createOrReplaceTempView(\u0026#34;catalog_sales\u0026#34;) spark.read.parquet(\u0026#34;s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/web_sales/\u0026#34;).createOrReplaceTempView(\u0026#34;web_sales\u0026#34;) In each notebook cell, you can open Spark Job Progress to view the stages of jobs sent to EMR Serverless for a specific cell. You can see the completion time for each stage. If errors occur, you can check the logs, making troubleshooting more seamless. Since the files are partitioned based on date key columns, you can observe that Spark runs tasks in parallel to read the data. 8. Next, get counts by date keys on data partitioned by time key using the following code:\nselect count(1), ss_sold_date_sk from store_sales group by ss_sold_date_sk order by ss_sold_date_sk Monitoring jobs in Spark UI In the Jobs tab of the Spark UI, you can view a list of completed or running jobs, with the following information:\nAction that triggered the job Execution time (e.g., 41 seconds, but times will vary) Number of stages and tasks ‚Äî in this example, 2 stages and 3,428 tasks You can select a job to see more details, especially about the stages. Our job has two stages; a new stage is created each time there is a shuffle. We have one stage to read data from each initial dataset, and one stage for aggregation. In the next example, we run some TPC-DS SQL queries used for performance evaluation and benchmarking:\nwith frequent_ss_items as (select substr(i_item_desc,1,30) itemdesc,i_item_sk item_sk,d_date solddate,count(*) cnt from store_sales, date_dim, item where ss_sold_date_sk = d_date_sk and ss_item_sk = i_item_sk and d_year in (2000, 2000+1, 2000+2,2000+3) group by substr(i_item_desc,1,30),i_item_sk,d_date having count(*) \u0026gt;4), max_store_sales as (select max(csales) tpcds_cmax from (select c_customer_sk,sum(ss_quantity*ss_sales_price) csales from store_sales, customer, date_dim where ss_customer_sk = c_customer_sk and ss_sold_date_sk = d_date_sk and d_year in (2000, 2000+1, 2000+2,2000+3) group by c_customer_sk) x), best_ss_customer as (select c_customer_sk,sum(ss_quantity*ss_sales_price) ssales from store_sales, customer where ss_customer_sk = c_customer_sk group by c_customer_sk having sum(ss_quantity*ss_sales_price) \u0026gt; (95/100.0) * (select * from max_store_sales)) select sum(sales) from (select cs_quantity*cs_list_price sales from catalog_sales, date_dim where d_year = 2000 and d_moy = 2 and cs_sold_date_sk = d_date_sk and cs_item_sk in (select item_sk from frequent_ss_items) and cs_bill_customer_sk in (select c_customer_sk from best_ss_customer) union all (select ws_quantity*ws_list_price sales from web_sales, date_dim where d_year = 2000 and d_moy = 2 and ws_sold_date_sk = d_date_sk and ws_item_sk in (select item_sk from frequent_ss_items) and ws_bill_customer_sk in (select c_customer_sk from best_ss_customer))) x You can monitor Spark jobs in SageMaker Unified Studio in two ways. Jupyter notebooks provide basic monitoring, showing real-time job status and execution progress. For more detailed analysis, use the Spark UI. You can check specific stages, tasks, and execution plans. The Spark UI is particularly useful for troubleshooting performance issues and optimizing queries. You can track the expected number of stages, running tasks, and detailed duration of each task. This comprehensive view helps you understand resource usage and track job progress at a detailed level.\nIn this section, we explained how you can use EMR Serverless compute in SageMaker Unified Studio to build interactive Spark applications. Through the Spark UI, interactive applications provide detailed task-level status, I/O and shuffle information, as well as links to corresponding task logs directly from the notebook, enabling a seamless debugging experience.\nCleanup To avoid ongoing charges in your AWS account, delete the resources you created in this tutorial:\nDelete connections Delete EMR jobs Delete EMR output S3 buckets Delete Amazon MWAA resources such as workflows and environments Conclusion In this post, we demonstrated how the next generation of SageMaker, combined with EMR Serverless, provides a powerful solution for developing, monitoring, and scheduling Spark applications using data in Amazon S3. The integrated experience significantly reduces complexity by providing a unified development environment, automatic resource management, and comprehensive monitoring capabilities through the Spark UI, while maintaining cost efficiency with the pay-as-you-use model. For enterprises, this means faster time to insights, improved team collaboration, and reduced operational overhead, allowing data teams to focus on analytics rather than infrastructure management.\nTo get started, explore the Amazon SageMaker Unified Studio User Guide, set up a project in your AWS environment, and discover how this solution can transform your organization\u0026rsquo;s data analytics capabilities.\nAppendix In the following sections, we discuss how to run scheduled workloads and provide details about the TPC-DS dataset for building Spark applications using EMR Serverless.\nRunning scheduled workloads In this section, we deploy JupyterLab notebooks and create workflows using Amazon MWAA. You can use workflows to orchestrate notebooks, querybooks, and many other things in your project repository. With workflows, you can define a set of tasks organized as a directed acyclic graph (DAG) that can run on a schedule you define. Implementation steps:\nIn SageMaker Unified Studio, choose Build, and under Orchestration, choose Workflows Choose Create Workflow in Editor You will be directed to the JupyterLab notebook with a new DAG named untitled.py created in the /src/workflows/dag folder\nRename this notebook to tpcds_data_queries.py\nYou can reuse the existing template with the following updates:\na. Update line 17 with the schedule you want your code to run on b. Update line 26 with your NOTEBOOK_PATH. This path should be in src/\u0026lt;notebook_name\u0026gt;.ipynb. Note that the dag_id name is auto-generated; you can name it as per your requirement. 5. Choose File and Save notebook To test, you can trigger a manual workload run 6. In SageMaker Unified Studio, choose Build, then under Orchestration, choose Workflows. 7. Choose your workflow, then choose Run. You can monitor job success on the Runs tab. To debug notebook jobs by accessing the Spark UI in the Airflow job console, you must use EMR Serverless Airflow Operators to submit jobs. The link is available on the Details tab of the query. This option has key limitations: it is not available for Amazon EMR on EC2, and SageMaker notebook job operators do not work.\nYou can configure the operator to create one-time links to the application UI and Spark output logs by passing enable_application_ui_links=True as a parameter. After the job starts running, these links are available on the Details tab of the corresponding task. If enable_application_ui_links=False, the links will appear but in a grayed-out state.\nMake sure you have emr-serverless:GetDashboardForJobRun in AWS Identity and Access Management (IAM) to create dashboard links.\nOpen the Airflow user interface for your task. The Spark user interface and history server dashboard options will appear on the Details tab, as shown in the following screenshot.\nThe screenshot illustrates the Jobs tab of the Spark UI.\nAbout the authors Amit Maindola is a Senior Data Architect focused on data engineering, analytics, and AI/ML at Amazon Web Services. He helps customers in their digital transformation journey and enables them to build highly scalable, robust, and secure cloud-based analytical solutions on AWS to gain timely insights and make critical business decisions.\nAbhilash Nagilla is a senior specialist solutions architect at Amazon Web Services (AWS), supporting public sector customers on their cloud journey with a focus on AWS data and AI services. Outside of work, Abhilash enjoys learning new technologies, watching movies, and traveling to new places.\n"},{"uri":"https://github.com/nhutruong47/report/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #2 - DevOps on AWS\u0026rdquo; Event Objectives Learn DevOps culture and principles in cloud environment Build complete CI/CD pipeline with AWS DevOps Services Master Infrastructure as Code (IaC) with CloudFormation and CDK Understand Container Services on AWS (ECR, ECS, EKS, App Runner) Implement monitoring and observability with CloudWatch and X-Ray Apply DevOps best practices and learn from real case studies Agenda-based Highlights 8:30 ‚Äì 9:00 | Welcome \u0026amp; DevOps Mindset Main activities:\nCheck-in and recap of previous AI/ML session Introduction to DevOps culture and principles Benefits and key metrics (DORA, MTTR, deployment frequency) Importance of DevOps in digital transformation Key takeaway: Understood that DevOps is not just tools but a working culture, grasped key metrics to measure DevOps effectiveness and how to apply them in practice.\n9:00 ‚Äì 10:30 | AWS DevOps Services ‚Äì CI/CD Pipeline Main content:\nSource Control: AWS CodeCommit and Git strategies (GitFlow, Trunk-based) Build \u0026amp; Test: CodeBuild configuration, automated testing pipelines Deployment: CodeDeploy with Blue/Green, Canary, and Rolling updates Orchestration: Automation with CodePipeline Live Demo: Complete CI/CD pipeline walkthrough Key takeaway: Mastered the process of building CI/CD pipeline from source code to deployment, understood different deployment strategies, and gained hands-on experience with AWS DevOps tools.\n10:30 ‚Äì 10:45 | Break Networking and discussion with AWS experts and other participants.\n10:45 ‚Äì 12:00 | Infrastructure as Code (IaC) Main content:\nAWS CloudFormation: Templates, stacks, and drift detection AWS CDK (Cloud Development Kit): Constructs, reusable patterns, and language support Demo: Deployment with CloudFormation and CDK Discussion: Choosing appropriate IaC tools Key takeaway: Deep understanding of Infrastructure as Code, comparison between CloudFormation and CDK, and learned how to apply IaC to manage infrastructure efficiently and reusably.\n12:00 ‚Äì 13:00 | Lunch Break (Self-arranged) 13:00 ‚Äì 14:30 | Container Services on AWS Main content:\nDocker Fundamentals: Microservices and containerization Amazon ECR: Image storage, scanning, lifecycle policies Amazon ECS \u0026amp; EKS: Deployment strategies, scaling, and orchestration AWS App Runner: Simplified container deployment Demo \u0026amp; Case Study: Microservices deployment comparison Key takeaway: Mastered containerization knowledge, understood differences between ECS and EKS, learned container image management with ECR, and experienced real microservices deployment.\n14:30 ‚Äì 14:45 | Break 14:45 ‚Äì 16:00 | Monitoring \u0026amp; Observability Main content:\nCloudWatch: Metrics, logs, alarms, and dashboards AWS X-Ray: Distributed tracing and performance insights Demo: Comprehensive observability setup Best Practices: Alerting, dashboards, and on-call processes Key takeaway: Learned how to set up effective monitoring and observability, understood how to use CloudWatch and X-Ray to monitor and debug applications, and applied best practices in alerting.\n16:00 ‚Äì 16:45 | DevOps Best Practices \u0026amp; Case Studies Main content:\nDeployment strategies: Feature flags, A/B testing Automated testing and CI/CD integration Incident management and postmortems Case Studies: DevOps transformation at startups and enterprises Key takeaway: Learned advanced deployment strategies, how to integrate automated testing, professional incident management processes, and real-world experience from case studies.\n16:45 ‚Äì 17:00 | Q\u0026amp;A \u0026amp; Wrap-up Main content:\nDevOps career pathways and professional opportunities AWS certification roadmap for DevOps Engineers Open discussion and Q\u0026amp;A session Key Takeaways Comprehensive understanding of DevOps culture and its implementation on AWS Proficient in building CI/CD pipeline with AWS CodeCommit, CodeBuild, CodeDeploy, CodePipeline Mastery of Infrastructure as Code with CloudFormation and CDK Deep understanding of container services: ECR, ECS, EKS, App Runner Learned to set up monitoring and observability with CloudWatch and X-Ray Applied DevOps best practices: deployment strategies, automated testing, incident management Real-world experience from case studies and demos Work Applications Build CI/CD pipeline for current projects Apply Infrastructure as Code to manage infrastructure Implement containerization for microservices architecture Set up effective monitoring and alerting system Apply deployment strategies like Blue/Green, Canary deployment Improve incident management and postmortem processes Event Experience Participating in \u0026ldquo;DevOps on AWS\u0026rdquo; was an extremely valuable experience, helping me gain deeper understanding of how to implement DevOps practices in AWS cloud environment.\nHighlights Learning from AWS experts: Gained access to knowledge from experienced Solution Architects, understanding DevOps mindset and practical applications. Detailed hands-on demos: Directly observed the complete CI/CD pipeline construction process, Infrastructure as Code demos with CloudFormation and CDK. Real-world case studies: Learned from successful DevOps implementation cases at both startups and enterprises. Quality networking: Opportunities to discuss with DevOps Engineers and Cloud Architects, sharing experiences and workplace challenges. Important Lessons DevOps is a journey, not a destination - need to apply step by step and continuously improve Infrastructure as Code is an important foundation for scalable DevOps Monitoring and observability are key to maintaining system reliability Container technology is the future of application deployment Event Images This event not only provided technical knowledge but also helped me understand the importance of DevOps culture in improving collaboration between Development and Operations teams, thereby enhancing product quality and delivery speed.\n"},{"uri":"https://github.com/nhutruong47/report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Set up Hybrid DNS with Route 53 Resolver. Set up VPC peering Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Deploy Amazon EC2 instances and core networking + Create EC2 server and test connectivity + Configure NAT Gateway and use Reachability Analyzer + Create EC2 Instance Connect Endpoint and use Systems Manager Session Manager + Enable CloudWatch monitoring and alerts + Set up Site-to-Site VPN (create VGW, CGW, VPN connection) and configure customer gateway + Modify VPN tunnels, explore alternative VPN setups, and troubleshoot VPN issues 15/09/2025 15/09/2025 https://000003.awsstudygroup.com/ 2 - Build VPN connection using Strongswan and Transit Gateway + Create Transit Gateway and attachments + Configure route tables and customer gateway + Clean up resources after testing + Set up Hybrid DNS with Route 53 Resolver and review Route 53 basics 16/09/2025 16/09/2025 https://000003.awsstudygroup.com/ https://000004.awsstudygroup.com/ 3 - Prepare Amazon Route 53 and related infrastructure + Generate key pairs and initialize CloudFormation templates + Configure security groups and connect to RDGW + Deploy Microsoft AD and set up DNS + Create Route 53 outbound/inbound endpoints and resolver rules + Test DNS resolution and clean up resources 17/09/2025 17/09/2025 https://000004.awsstudygroup.com/ 4 - Set up VPC peering and cross-VPC connectivity + Review prerequisites and initialize CloudFormation templates + Create security groups and EC2 instances as needed + Update Network ACLs, configure route tables and enable cross-peer DNS + Cleanup resources after validation 18/09/2025 18/09/2025 https://000019.awsstudygroup.com/ 5 - Hands-on practices: + Create and configure EC2 server + Test connectivity to EC2 instances + Set up Hybrid DNS with Route 53 Resolver + Explore Amazon Route 53 features + Generate key pairs and validate readiness for Route 53 19/09/2025 19/09/2025 üèÜ Week 2 Achievements 1. EC2 Deployment \u0026amp; Core Networking\nDeployed Amazon EC2 instances and validated connectivity. Configured NAT Gateway and used Reachability Analyzer to verify routing. Enabled EC2 Instance Connect and Systems Manager Session Manager for access; set up CloudWatch monitoring and alerts. 2. Site-to-Site VPN\nEstablished Site-to-Site VPN components (Virtual Private Gateway, Customer Gateway and VPN connection). Modified and tested VPN tunnels; applied troubleshooting steps to resolve connectivity issues. 3. Transit Gateway \u0026amp; StrongSwan VPN\nBuilt VPN using StrongSwan with Transit Gateway; created TGW and attachments. Configured route tables and customer gateway settings; validated cross-VPC routing. 4. Route 53 \u0026amp; Microsoft AD\nDeployed Microsoft AD and configured Route 53: created inbound/outbound endpoints and resolver rules. Tested DNS resolution across environments and verified connectivity. 5. VPC Peering \u0026amp; Hands-on Practice\nImplemented VPC peering and configured cross-peer DNS; validated networking and DNS across peered VPCs. Completed hands-on tasks: EC2 setup, connectivity testing, Hybrid DNS configuration, and key-pair validation. "},{"uri":"https://github.com/nhutruong47/report/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Create an S3 Interface endpoint","tags":[],"description":"","content":"In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"},{"uri":"https://github.com/nhutruong47/report/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "},{"uri":"https://github.com/nhutruong47/report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Proposal ‚Äì Smart Resume Analyzer A Unified AWS Serverless solution to analyze CVs vs JDs and generate Fit Scores\nNote: This proposal follows the sectioning style of your previous _index.md sample but is rewritten for the Smart Resume Analyzer project.\n1) Executive Summary Smart Resume Analyzer is a serverless web platform that evaluates the match between a candidate‚Äôs CV and a Job Description (JD). It calculates a Fit Score, detects skill gaps, and provides personalized learning suggestions.\nThe solution is implemented by a 5‚Äëmember team in 4 weeks on AWS using managed, pay‚Äëas‚Äëyou‚Äëgo services to keep costs near zero for a demo workload. The UI is built with Next.js and hosted on AWS Amplify; the backend uses API Gateway + Lambda with DynamoDB, S3, Comprehend, Textract, and Cognito.\nKey outcomes\n90% faster CV screening for demo scenarios. Objective Fit Score with visual reports. Actionable learning roadmap per candidate. 2) Problem Statement 2.1 What‚Äôs the problem? Recruiters spend significant time manually reading CVs and comparing them to JDs. Candidates lack insight into which skills they are missing and how to improve. Existing tools are expensive or not tailored for Vietnamese/SEA use cases. 2.2 The solution Upload CV (PDF/DOCX) and JD ‚Üí automatic text extraction and NLP. Detect skills, experience, education; compute Fit Score vs JD. Recommend skill pathways mapped from a small SkillOntology store. Secure login with Cognito; results shown in a clean Next.js dashboard. 3) Solution Architecture (overview) Serverless, event‚Äëdriven architecture on AWS.\nMain components\nFrontend: Next.js UI (Amplify Hosting) for upload \u0026amp; result dashboard. API Layer: Amazon API Gateway ‚Üí AWS Lambda functions. Processing: parseResume ‚Üí Textract (if scanned PDF) ‚Üí normalized text. nlpAnalyze ‚Üí Comprehend ‚Üí entities/skills/phrases. recommendSkills ‚Üí compares to JD + SkillOntology in DynamoDB. Data: DynamoDB (results, ontology), S3 (temporary CV/JD). Identity: Cognito (JWT access tokens). Ops: IaC with AWS SAM, CI/CD via CodeBuild + CodePipeline, logging in CloudWatch. (A Mermaid architecture diagram is provided separately.)\n4) Technical Implementation 4.1 Tech stack Backend: .NET 8 (C# Minimal API on Lambda) Frontend: Next.js + TailwindCSS (Amplify Hosting) AWS: Lambda, API Gateway, DynamoDB, S3, Cognito, Comprehend, Textract IaC: AWS SAM CI/CD: CodeBuild + CodePipeline 4.2 End‚Äëto‚Äëend flow User authenticates via Cognito and obtains JWT. Frontend requests presigned URL to S3 ‚Üí uploads CV/JD. API Gateway invokes Lambda parseResume: If PDF scan ‚Üí Textract ‚Üí extract text; otherwise direct parse. Clean \u0026amp; normalize ‚Üí store interim artifacts on S3. Lambda nlpAnalyze uses Comprehend to detect entities/skills ‚Üí writes results to DynamoDB. Lambda recommendSkills loads SkillOntology from DynamoDB ‚Üí compares CV vs JD ‚Üí computes Fit Score + gaps. Frontend queries results via API ‚Üí renders charts/tables. 4.3 Data model (DynamoDB ‚Äì simplified) Table Profiles (PK: userId, SK: profileId) ‚Äì store latest CV parse. Table Analyses (PK: analysisId) ‚Äì fit score, gaps, timestamps. Table SkillOntology (PK: skillId, attributes: name, tags, learningPath[]). 4.4 API (high level) POST /upload-url ‚Üí presign for CV/JD. POST /analyze ‚Üí triggers pipeline for a given S3 key pair. GET /analyses/{id} ‚Üí returns Fit Score \u0026amp; recommendations. GET /skills/{id} ‚Üí (optional) fetch a skill‚Äôs learning path. 5) Timeline \u0026amp; Milestones (4 weeks) Week Milestone Deliverables 1 Foundation SAM template, DynamoDB tables, Cognito, base UI 2 Parsing \u0026amp; NLP parseResume, nlpAnalyze, JD parsing, unit tests 3 Recommender \u0026amp; FE integration recommendSkills, dashboard, charts 4 Demo \u0026amp; hardening E2E tests, logging, cost tuning, slide deck 6) Budget Estimation (demo scale) Indicative, assuming \u0026lt; 500 requests/month\nLambda: ~$0.02 API Gateway: ~$0.01 S3 (few GB, low requests): ~$0.10 DynamoDB (on‚Äëdemand, low R/W): ~$0.05 Amplify Hosting: ~$0.30 Comprehend + Textract (small pages): ~$0.40 Cognito: $0.00\nTotal ‚âà $0.9 / month (~$10 / year) 7) Security, Risks \u0026amp; Mitigations Security\nPrivate S3 buckets with SSE‚ÄëKMS; presigned uploads only. IAM least privilege; API protected by Cognito JWT. PII masking for logs; CloudWatch alarms. Optional: set lifecycle rules to delete raw CV/JD after analysis. Risks \u0026amp; mitigations\nNLP accuracy: Provide supported formats + fallback to keyword rules. Large/unclean CVs: Validate size/format; sanitize before NLP. Cost spikes: AWS Budget alarms; cap page counts per request. 8) Expected Outcomes Automated CV‚ÄëJD matching with transparent Fit Score. Visual breakdown of skills matched vs gaps and learning roadmap. Serverless, low‚Äëops stack that‚Äôs easy to demo, extend, and localize. üìÑ Proposal Document (Google Docs) üëâ Review Proposal here:\nGOOGLE DOC LINK\n"},{"uri":"https://github.com/nhutruong47/report/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Test the Gateway Endpoint","tags":[],"description":"","content":"Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"},{"uri":"https://github.com/nhutruong47/report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Democratize data for timely decisions with text-to-SQL at Parcel Perform by Yudho Ahmad Diponegoro, Le Vy, and Jun Kai Loke | on July 09, 2025 | in Amazon Athena, Amazon Bedrock, Amazon Bedrock Knowledge Bases, Business Intelligence, Customer Solutions, Generative AI, Intermediate (200), Supply Chain\nThis post is co-written with Le Vy from Parcel Perform. Access to accurate data is often the true differentiator between excellent decisions and timely decisions. This becomes even more critical for customer-facing decisions and actions. A modern AI deployed correctly can help your organization simplify data access to make accurate and timely decisions for customer-facing business teams, while minimizing the undifferentiated heavy lifting that your data team has to do. In this post, we share how Parcel Perform, a leading AI Delivery Experience platform for global ecommerce businesses, has implemented such a solution.\nAccurate post-purchase deliveries tracking can be crucial for many ecommerce merchants. Parcel Perform provides an AI-driven, intelligent end-to-end data and delivery experience and software as a service (SaaS) system for ecommerce merchants. The system uses AWS services and state-of-the-art AI to process hundreds of millions of daily parcel delivery movement data and provide a unified tracking capability across couriers for the merchants, with emphasis on accuracy and simplicity.\nThe business team in Parcel Perform often needs access to data to answer questions related to merchants‚Äô parcel deliveries, such as ‚ÄúDid we see a spike in delivery delays last week? If so, in which transit facilities were this observed, and what was the primary cause of the issue?‚Äù Previously, the data team had to manually form the query and run it to fetch the data. With the new generative AI-powered text-to-SQL capability in Parcel Perform, the business team can self-serve their data needs by using an AI assistant interface. In this post, we discuss how Parcel Perform incorporated generative AI, data storage, and data access through AWS services to make timely decisions.\nData analytics architecture The solution starts with data ingestion, storage, and access. Parcel Perform adopts a data analytics architecture as shown in the following diagram.\nOne key data type in the Parcel Perform parcel monitoring application is the parcel event data, which can reach billions of rows. This includes the parcel‚Äôs shipment status change, location change, and much more. This day-to-day data from multiple business units lands in relational databases hosted on Amazon Relational Database Service (Amazon RDS).\nAlthough relational databases are suitable for rapid data ingestion and consumption from the application, a separate analytics stack is needed to handle analytics in a scalable and performant way without disrupting the main application. These analytics needs include answering aggregation queries from questions like ‚ÄúHow many parcels were delayed last week?‚Äù\nParcel Perform uses Amazon Simple Storage Service (Amazon S3) with a query engine provided by Amazon Athena to meet their analytics needs. With this approach, Parcel Perform benefits from cost-effective storage while still being able to run SQL queries as needed on the data through Athena, which is priced on usage.\nData in Amazon S3 is stored in Apache Iceberg data format that allows data updates, which is useful in this case because the parcel events sometimes get updated. It also supports partitioning for better performance Amazon S3 Tables, launched in late 2024, is a feature for managing Iceberg tables, and could also be an option for you.\nParcel Perform uses an Apache Kafka cluster managed by Amazon Managed Streaming for Apache Kafka (Amazon MSK) as a stream to transfer data from source to S3 bucket. Amazon MSK Connect with Debezium connector streams data using change data capture (CDC) from Amazon RDS to Amazon MSK.\nApache Flink, running on Amazon Elastic Kubernetes Service (Amazon EKS), processes the data streams from Amazon MSK. It writes this data to the S3 bucket in Iceberg format, and updates the data schema in AWS Glue Data Catalog. This data schema allows Athena to query the correct data in the S3 bucket.\nNow that you understand how data is ingested and stored, we\u0026rsquo;ll look at how data is consumed through a data serving assistant using generative AI for business teams at Parcel Perform.\nData-queryable AI agent The users of the data serving AI agent at Parcel Perform are customer-facing business team members who regularly query parcel event data to answer questions from ecommerce merchants about deliveries and support them proactively. The following screenshot shows the AI assistant UI experience, powered by text-to-SQL with generative AI.\nThis functionality helped the Parcel Perform team and their customers save time, which we discuss later in this post. In the following section, we present the architecture that powers this feature.\nText-to-SQL AI agent architecture The data serving AI assistant architecture in Parcel Perform is shown in the following diagram.\nThe AI assistant UI is supported by an application built with the FastAPI framework hosted on Amazon EKS. It is also fronted by an Application Load Balancer to allow for potential horizontal scalability.\nThe application uses LangGraph to orchestrate the workflow of large language model (LLM) calls, tool usage, and memory checkpointing. The graph uses multiple tools, including tools from the SQLDatabase Toolkit to automatically retrieve data schema through Athena. The graph also uses an Amazon Bedrock Knowledge Bases retriever to retrieve business information from a knowledge base. Parcel Perform uses Anthropic\u0026rsquo;s Claude models in Amazon Bedrock to generate SQL.\nAlthough the function of Athena as a query engine to query the parcel event data on Amazon S3 is clear, Parcel Perform still needs a knowledge base. In this use case, the SQL generation performs better when the LLM has more business contextual information to help interpret database fields and translate logistics terminology into data representations. This is better illustrated with the following two examples:\nParcel Perform‚Äôs data lake operations use specific codes c for create and u for update. When analyzing data, Parcel Perform sometimes needs to focus only on initial creation records, where operation code is equal to c. Because this business logic might not be inherent in the training of LLMs in general, Parcel Perform explicitly defines this in their business context.\nIn logistics terminology, transit time has specific industry conventions. It‚Äôs measured in days, and same-day deliveries are recorded as transit_time = 0. Although this is intuitive for logistics professionals, an LLM might incorrectly interpret a request like ‚ÄúGet me all shipments with same-day delivery‚Äù by usingWHERE transit_time = 1 instead of WHERE transit_time = 0 in the generated SQL.\nTherefore, each incoming question goes to a Retrieval Augmented Generation (RAG) workflow to find potentially relevant stored business information, to enrich the context. This mechanism helps provide the specific rules and interpretations that even advanced LLMs might not be able to derive from general training data.\nParcel Perform uses Amazon Bedrock Knowledge Bases as a managed solution for the RAG workflow. They ingest business context information by uploading files to Amazon S3. Amazon Bedrock Knowledge Bases processes the files, chunks them, uses embedding models to create vectors, and stores the vectors in a vector database so they can be searched. These steps are fully managed by Amazon Bedrock Knowledge Bases. Parcel Perform stores the vectors in Amazon OpenSearch Serverless as the chosen vector database to simplify infrastructure management.\nAmazon Bedrock Knowledge Bases provides the Retrieve API, which takes in an input (such as a question from the AI assistant), converts it into a vector embedding, searches for relevant chunks of business context information in the vector database, and returns the top relevant document chunks. It is integrated with the LangChain Amazon Bedrock Knowledge Bases retriever by calling the invoke method.\nThe next step involves invoking an AI agent with the supplied business contextual information and the SQL generation prompt. The prompt was inspired by a prompt in LangChain Hub. The following is a code snippet of the prompt:\nYou are an agent designed to interact with a SQL database. Given an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results. Relevant context: {rag_context} You can order the results by a relevant column to return the most interesting examples in the database. Never query for all the columns from a specific table, only ask for the relevant columns given the question. You have access to tools for interacting with the database. - Only use the below tools. Only use the information returned by the below tools to construct your final answer. - DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database. - To start querying for final answer you should ALWAYS look at the tables in the database to see what you can query. Do NOT skip this step. - Then you should query the schema of the most relevant tables The prompt sample is part of the initial instruction for the agent. The data schema is automatically inserted by the tools from the SQLDatabase Toolkit at a later step of this agentic workflow. The following steps occur after a user enters a question in the AI assistant UI:\nThe question triggers a LangGraph execution run.\nThe following processes happen in parallel: a. The graph fetches the database schema from Athena through SQLDatabase Toolkit. b. The graph passes the question to the Amazon Bedrock Knowledge Bases retriever and gets a list of relevant business information regarding the question.\nThe graph invokes an LLM using Amazon Bedrock by passing the question, the conversation context, data schema, and business context information. The result is the generated SQL.\nThe graph uses the SQLDatabase Toolkit again to run the SQL through Athena and receive data results.\nThe data output is passed into an LLM to generate the final response based on the initial question asked. Amazon Bedrock Guardrails is used as a safeguard to avoid inappropriate inputs and responses.\nThe final response is returned to the user through the AI assistant UI.\nThe following diagram illustrates these steps.\nThis implementation demonstrates how Parcel Perform transforms raw inquiries into actionable data for timely decision-making. Security is also implemented in multiple components. From a network perspective, the EKS pods are placed in private subnets in Amazon Virtual Private Cloud (Amazon VPC) to improve network security of the AI assistant application. This AI agent is placed behind a backend layer that requires authentication. For data security, sensitive data is masked at rest in the S3 bucket. Parcel Perform also limits the permissions of the AWS Identity and Access Management (IAM) role used to access the S3 bucket so it can only access certain tables.\nIn the following sections, we discuss how Parcel Perform approached building this data transformation solution.\nFrom idea to production Parcel Perform started with the idea of freeing their data team from manually serving the request from the business team, while also improving the timeliness of the data availability to support the business team‚Äôs decision-making.\nWith the help of the AWS Solutions Architect team, Parcel Perform completed a proof of concept using AWS services and a Jupyter notebook in Amazon SageMaker Studio. After an initial success, Parcel Perform integrated the solution with their orchestration tool of choice, LangGraph.\nBefore going into production, Parcel Perform conducted thorough testing to verify result consistency. They added LangSmith Tracing to record the steps and results of the AI agent to evaluate its performance.\nThe Parcel Perform team discovered challenges during their journey, which we discuss in the following section. They performed prompt engineering to address those challenges. Eventually, the AI agent was integrated into production to be used by the business team. Afterward, Parcel Perform collected user feedback internally and monitored logs from LangSmith Tracing to verify performance was maintained.\nChallenges This journey was not immune to challenges.\nThis journey isn‚Äôt free from challenges. Firstly, some ecommerce merchants might have several records in the data lake under various names. For example, a merchant with the name ‚ÄúABC‚Äù might have multiple records such, as ‚ÄúABC Singapore Holdings Pte. Ltd.,‚Äù ‚ÄúABC Demo Account,‚Äù ‚ÄúABC Test Group,‚Äù and so on. For a question like ‚ÄúWas there any parcel shipment delay by ABC last week?‚Äù, the generated SQL has the element of WHERE merchant_name LIKE '%ABC%' which might result in ambiguity. During the proof of concept stage, this problem caused incorrect matching of the result.\nFor this challenge, Parcel Perform relies on careful prompt engineering to instruct the LLM to identify when the name was potentially ambiguous. The AI agent then calls Athena again to look for matching names. The LLM decides which merchant name to use based on multiple factors, including the significance in data volume contribution and the account status in the data lake. In the future, Parcel Perform intends to implement a more sophisticated technique by prompting the user to resolve the ambiguity.\nThe second challenge is about unrestricted questions that might yield expensive queries running across large amounts of data and resulting in longer query waiting time. Some of these questions might not have a LIMIT clause imposed in the query. To solve this, Parcel Perform instructs the LLM to add a LIMIT clause with a certain number of maximum results if the user doesn‚Äôt specify the intended number of results. In the future, Parcel Perform plans to use the query EXPLAIN results to identify heavy queries.\nThe third challenge is related to tracking usage and incurred cost of this particular solution. Having started multiple generative AI projects using Amazon Bedrock and sometimes with the same LLM ID, Parcel Perform must distinguish usage incurred by projects. Parcel Perform creates an inference profile for each project, associates the profile with tags, and includes that profile in each LLM call for that project. With this setup, Parcel Perform is able to segregate costs based on projects to improve cost visibility and monitoring.\nThe impact To extract data, the business team clarifies details with the data team, makes a request, checks feasibility, and waits for bandwidth. This process lengthens when requirements come from customers or teams in different time zones, with each clarification adding 12‚Äì24 hours due to asynchronous communication. Simpler requests made early in the workday might complete within 24 hours, whereas more complex requests or those during busy periods can take 3‚Äì5 business days.\nWith the text-to-SQL AI agent, this process is dramatically streamlined‚Äîminimizing the back-and-forth communication for requirement clarification, removing the dependency on data team bandwidth, and automating result interpretation.\nParcel Perform‚Äôs measurements show that the text-to-SQL AI agent reduces the average time-to-insight by 99%, from 2.3 days to an average of 10 minutes, saving approximately 3,850 total hours of wait time per month across requesters while maintaining data accuracy.\nUsers can directly query the data without intermediaries, receiving results in minutes rather than days. Teams across time zones can now access insights any time of day, alleviating the frustrating ‚Äúwait until Asia wakes up‚Äù or ‚Äúcatch EMEA before they leave‚Äù delays, leading to happier customers and faster problem-solving.\nThis transformation has profoundly impacted the data analytics team‚Äôs capacity and focus, freeing the data team for more strategic work and helping everyone make faster, more informed decisions. Before, the analysts spent approximately 25% of their working hours handling routine data extraction requests‚Äîequivalent to over 260 hours monthly across the team. Now, with basic and intermediate queries automated, this number has dropped to just 10%, freeing up nearly 160 hours each month for high-impact work. Analysts now focus on complex data analysis rather than spending time on basic data retrieval tasks.\nConclusion Parcel Perform‚Äôs solution demonstrates how you can use generative AI to enhance productivity and customer experience. Parcel Perform has built a text-to-SQL AI agent that transforms a business team‚Äôs question into SQL that can fetch the actual data. This improves the timeliness of data availability for decision-making that involves customers. Furthermore, the data team can avoid the undifferentiated heavy lifting to focus on complex data analysis tasks.\nThis solution uses multiple AWS services like Amazon Bedrock and tools like LangGraph. You can start with a proof of concept and consult your AWS Solutions Architect or engage with AWS Partners. If you have questions, post them on AWS re:Post. You can also make the development more straightforward with the help of Amazon Q Developer. When you face challenges, you can iterate to find the solution, which might include prompt engineering or adding additional steps to your workflow.\nSecurity is a top priority. Make sure your AI assistant has proper guardrails in place to protect against prompt threats, inappropriate topics, profanity, leaked data, and other security issues. You can integrate Amazon Bedrock Guardrails with your generative AI application through an API.To learn more, refer to the following resources:\nBuild a robust text-to-SQL solution generating complex queries, self-correcting, and querying diverse data sources X√¢y d·ª±ng gi·∫£i ph√°p chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n sang SQL m·∫°nh m·∫Ω, t·∫°o ra c√°c truy v·∫•n ph·ª©c t·∫°p, t·ª± ƒë·ªông s·ª≠a l·ªói v√† truy v·∫•n c√°c ngu·ªìn d·ªØ li·ªáu ƒëa d·∫°ng LangGraph agents with Amazon Bedrock workshop Build a knowledge base by connecting to a structured data store About the authors Yudho Ahmad Diponegoro is a Senior Solutions Architect at AWS. Having been part of Amazon for 10+ years, he has had various roles from software development to solutions architecture. He helps startups in Singapore when it comes to architecting in the cloud. While he keeps his breadth of knowledge across technologies and industries, he focuses in AI and machine learning where he has been guiding various startups in ASEAN to adopt machine learning and generative AI at AWS.\nLe Vy is the AI Team Lead at Parcel Perform, where she drives the development of AI applications and explores emerging AI research. She started her career in data analysis and deepened her focus on AI through a Master‚Äôs in Artificial Intelligence. Passionate about applying data and AI to solve real business problems, she also dedicates time to mentoring aspiring technologists and building a supportive community for youth in tech. Through her work, Vy actively challenges gender norms in the industry and champions lifelong learning as a key to innovation.\nLoke Jun Kai is a GenAI/ML Specialist Solutions Architect in AWS, covering strategic customers across the ASEAN region. He works with customers ranging from Start-up to Enterprise to build cutting-edge use cases and scalable GenAI Platforms. His passion in the AI space, constant research and reading, have led to many innovative solutions built with concrete business outcomes. Outside of work, he enjoys a good game of tennis and chess.\n"},{"uri":"https://github.com/nhutruong47/report/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #3 - Well-Architected Security Pillar\u0026rdquo; Event Objectives Master the 5 core pillars of AWS Well-Architected Security Framework Understand modern security threats and best practices in cloud environment Learn comprehensive security implementation from identity management to incident response Apply security principles in real-world enterprise scenarios Event Agenda \u0026amp; Highlights 8:30 ‚Äì 8:50 AM | Opening \u0026amp; Security Foundation Main content:\nRole of Security Pillar in Well-Architected Framework Core principles: Least Privilege ‚Äì Zero Trust ‚Äì Defense in Depth Shared Responsibility Model understanding Top cloud security threats in Vietnam\u0026rsquo;s enterprise environment Key takeaway: Gained foundational understanding of AWS security philosophy and how shared responsibility model applies to different service layers. Learned about current security landscape and common vulnerabilities in Vietnamese cloud environments.\n8:50 ‚Äì 9:30 AM | Pillar 1 ‚Äî Identity \u0026amp; Access Management Main content:\nModern IAM Architecture: Users, Roles, Policies best practices Avoiding long-term credentials and implementing temporary access IAM Identity Center: SSO implementation and permission sets SCP \u0026amp; permission boundaries for multi-account governance MFA implementation, credential rotation strategies, Access Analyzer utilization Mini Demo: IAM Policy validation and access simulation Key takeaway: Mastered modern IAM patterns, understood the critical importance of eliminating long-term credentials, and gained hands-on experience with policy validation tools. Learned multi-account security governance strategies.\n9:30 ‚Äì 9:55 AM | Pillar 2 ‚Äî Detection \u0026amp; Continuous Monitoring Main content:\nCloudTrail organization-level logging and management GuardDuty threat detection and Security Hub centralization Comprehensive logging: VPC Flow Logs, ALB logs, S3 access logs Alerting and automation with EventBridge integration Detection-as-Code approach for infrastructure and security rules Key takeaway: Understood comprehensive monitoring strategy across all AWS layers, learned how to implement automated threat detection and response, and gained insight into treating security rules as code for better governance.\n9:55 ‚Äì 10:10 AM | Coffee Break Networking with security professionals and AWS experts.\n10:10 ‚Äì 10:40 AM | Pillar 3 ‚Äî Infrastructure Protection Main content:\nVPC segmentation strategies and network isolation Private vs public subnet placement best practices Security Groups vs NACLs: practical application models WAF + Shield + Network Firewall integration Workload protection: EC2, ECS/EKS security fundamentals Key takeaway: Mastered network security layering approach, understood when to use different network security controls, and learned workload-specific security implementations for containers and compute services.\n10:40 ‚Äì 11:10 AM | Pillar 4 ‚Äî Data Protection Main content:\nKMS: key policies, grants, and automated rotation Encryption at-rest \u0026amp; in-transit: S3, EBS, RDS, DynamoDB implementation Secrets Manager \u0026amp; Parameter Store rotation patterns Data classification frameworks and access guardrails Key takeaway: Deep understanding of AWS encryption services, learned practical patterns for secrets management and rotation, and understood how to implement data classification and protection strategies across different service types.\n11:10 ‚Äì 11:40 AM | Pillar 5 ‚Äî Incident Response Main content:\nIR lifecycle according to AWS best practices Practical playbooks for common scenarios: Compromised IAM credentials S3 public exposure incidents EC2 malware detection and response Snapshot creation, workload isolation, evidence collection procedures Automated response implementation using Lambda and Step Functions Key takeaway: Gained practical incident response skills with AWS-specific tools, learned automated response patterns, and understood how to implement security orchestration for faster incident resolution.\n11:40 ‚Äì 12:00 PM | Wrap-Up \u0026amp; Q\u0026amp;A Main content:\nComprehensive review of all 5 security pillars Common pitfalls and real-world challenges in Vietnamese enterprises Security learning roadmap: Security Specialty and Solutions Architect Pro certifications Key Takeaways Comprehensive Security Framework: Deep understanding of all 5 Well-Architected Security Pillars and their interconnections Modern IAM Practices: Mastery of identity and access management with zero-trust principles Detection \u0026amp; Response: Implementation of comprehensive monitoring and automated incident response Infrastructure Security: Network segmentation and workload protection strategies Data Protection: Encryption and secrets management best practices Practical Application: Real-world security scenarios and enterprise-specific challenges Work Applications Implement comprehensive security assessment using Well-Architected Security Pillar framework Redesign IAM architecture following modern best practices and eliminating long-term credentials Set up automated threat detection and response using GuardDuty, Security Hub, and EventBridge Implement network segmentation and workload protection for current infrastructure Establish data classification and encryption standards across all services Develop incident response playbooks and automation for common security scenarios Event Experience Participating in \u0026ldquo;AWS Cloud Mastery Series #3\u0026rdquo; on Well-Architected Security Pillar was an invaluable experience that provided comprehensive understanding of cloud security best practices and real-world implementation strategies.\nHighlights Expert-led Security Training: Learning from AWS security specialists with deep enterprise experience Hands-on Demonstrations: Practical demos of IAM policy validation, threat detection, and incident response Real-world Case Studies: Learning from actual security incidents and response scenarios Comprehensive Coverage: All 5 security pillars covered with practical implementation guidance Important Lessons Security is not a one-time implementation but a continuous process requiring constant monitoring and improvement Zero-trust architecture requires fundamental changes in how we approach identity and access management Automation is critical for effective security at scale, especially for detection and incident response Data protection must be implemented at every layer, from infrastructure to application level Event Images This security-focused event significantly enhanced my understanding of cloud security architecture and provided practical tools for implementing enterprise-grade security in AWS environments. The comprehensive coverage of all security pillars and hands-on approach made complex security concepts accessible and immediately applicable.\n"},{"uri":"https://github.com/nhutruong47/report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Set up AWS Transit Gateway Create Transit Gateway Attachments and Route Tables Learn Amazon EC2 comprehensive concepts and services Study EC2 Auto Scaling, EFS/FSx, Lightsail, and MGN Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Introduction to AWS Transit Gateway and lab overview + Understand Transit Gateway concepts + Compare VPC Peering and Transit Gateway + Review benefits and common use cases + Review lab architecture and prerequisites 22/09/2025 22/09/2025 https://000020.awsstudygroup.com/ 2 - Create and configure Transit Gateway + Set Transit Gateway parameters and settings + Verify Transit Gateway configuration 23/09/2025 23/09/2025 https://000020.awsstudygroup.com/ 3 - Create Transit Gateway attachments and attach VPCs + Configure attachment settings + Verify attachment status and connectivity 24/09/2025 24/09/2025 https://000020.awsstudygroup.com/ 4 - Configure Transit Gateway route tables and test connectivity + Create TGW route tables + Add routes to VPC route tables + Test inter-VPC connectivity + Clean up resources 25/09/2025 25/09/2025 https://000020.awsstudygroup.com/ 5 - Module 03-01: Amazon EC2 deep dive + EC2 instance families, types and sizing + AMI, backup strategies and key pair management + EBS vs Instance Store, snapshots and encryption + User data and metadata + EC2 Auto Scaling overview + Brief overview: EFS/FSx, Lightsail, MGN 26/09/2025 26/09/2025 üèÜ Week 3 Achievements 1. Transit Gateway Concepts \u0026amp; Design\nLearned core Transit Gateway concepts and reviewed the lab architecture. Compared Transit Gateway with VPC Peering and discussed typical use cases and trade-offs. 2. Transit Gateway Attachments\nCreated Transit Gateway attachments and attached VPCs. Verified attachment status and connectivity; validated VPC-to-TGW communications. 3. Route Tables \u0026amp; Testing\nCreated Transit Gateway route tables and populated routes into VPC route tables. Performed connectivity tests between VPCs and cleaned up test resources. 4. Amazon EC2 Fundamentals\nReviewed EC2 instance families, types and sizing considerations. Covered AMI usage, backup strategies, key pair management, EBS vs Instance Store, snapshots and encryption. Examined user data/metadata and an overview of EC2 Auto Scaling. "},{"uri":"https://github.com/nhutruong47/report/5-workshop/5.3-s3-vpc/","title":"Access S3 from VPC","tags":[],"description":"","content":"Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "},{"uri":"https://github.com/nhutruong47/report/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Test the Interface Endpoint","tags":[],"description":"","content":"Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "},{"uri":"https://github.com/nhutruong47/report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs you have translated. For example:\nBlog 1 - Develop and monitor a Spark application using existing data in Amazon S3 with Amazon SageMaker Unified Studio This blog explains how to develop and monitor Spark applications in the integrated environment of Amazon SageMaker Unified Studio. You will learn how to address challenges in managing big data analytics, such as fragmented development environments, complex resource management, and inconsistent monitoring. The article details how to set up Serverless EMR, develop Spark applications with the TPC-DS dataset in Jupyter notebooks, monitor performance using the Spark UI, and automate workflows with Amazon MWAA. This unified solution helps data teams focus on analytics instead of managing infrastructure.\nBlog 2 -Democratize data for timely decisions with text-to-SQL at Parcel Perform This blog explains how Parcel Perform uses generative AI and modern architecture to help operations teams query data quickly without relying on technical teams. It covers how to build a data lake to process billions of shipping event records per day, and how to use text-to-SQL and RAG workflows to automatically translate user questions into precise SQL queries.\nBlog 3 -Build a scalable AI video generator using Amazon SageMaker AI and CogVideoX This blog introduces how to build a scalable AI video generation system using Amazon SageMaker AI and the CogVideoX model. The solution transforms text and images into high-quality videos for a variety of purposes such as marketing, education, and product demonstrations, while leveraging AWS services for performance, security, and scalability.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://github.com/nhutruong47/report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Build a scalable AI video generator using Amazon SageMaker AI and CogVideoX by Nick Biso, Jinzhao Feng, Katherine Feng, and Natasha Tchir | on JUN 19 2025 | in Amazon SageMaker, Amazon SageMaker AI, Artificial Intelligence, Generative AI,Intermediate (200)\nIn recent years, the rapid advancement of artificial intelligence and machine learning (AI/ML) technologies has revolutionized various aspects of digital content creation. One particularly exciting development is the emergence of video generation capabilities, which offer unprecedented opportunities for companies across diverse industries. This technology allows for the creation of short video clips that can be seamlessly combined to produce longer, more complex videos. The potential applications of this innovation are vast and far-reaching, promising to transform how businesses communicate, market, and engage with their audiences. Video generation technology presents a myriad of use cases for companies looking to enhance their visual content strategies. For instance, ecommerce businesses can use this technology to create dynamic product demonstrations, showcasing items from multiple angles and in various contexts without the need for extensive physical photoshoots. In the realm of education and training, organizations can generate instructional videos tailored to specific learning objectives, quickly updating content as needed without re-filming entire sequences. Marketing teams can craft personalized video advertisements at scale, targeting different demographics with customized messaging and visuals. Furthermore, the entertainment industry stands to benefit greatly, with the ability to rapidly prototype scenes, visualize concepts, and even assist in the creation of animated content. The flexibility offered by combining these generated clips into longer videos opens up even more possibilities. Companies can create modular content that can be quickly rearranged and repurposed for different displays, audiences, or campaigns. This adaptability not only saves time and resources, but also allows for more agile and responsive content strategies. As we delve deeper into the potential of video generation technology, it becomes clear that its value extends far beyond mere convenience, offering a transformative tool that can drive innovation, efficiency, and engagement across the corporate landscape.\nIn this post, we explore how to implement a robust AWS-based solution for video generation that uses the CogVideoX model and Amazon SageMaker AI.\nSolution overview Our architecture delivers a highly scalable and secure video generation solution using AWS managed services. The data management layer implements three purpose-specific Amazon Simple Storage Service (Amazon S3) buckets‚Äîfor input videos, processed outputs, and access logging‚Äîeach configured with appropriate encryption and lifecycle policies to support data security throughout its lifecycle.\nFor compute resources, we use AWS Fargate for Amazon Elastic Container Service (Amazon ECS) to host the Streamlit web application, providing serverless container management with automatic scaling capabilities. Traffic is efficiently distributed through an Application Load Balancer. The AI processing pipeline uses SageMaker AI processing jobs to handle video generation tasks, decoupling intensive computation from the web interface for cost optimization and enhanced maintainability. User prompts are refined through Amazon Bedrock, which feeds into the CogVideoX-5b model for high-quality video generation, creating an end-to-end solution that balances performance, security, and cost-efficiency.\nThe following diagram illustrates the solution architecture.\nSolution overview CogVideoX is an open source, state-of-the-art text-to-video generation model capable of producing 10-second continuous videos at 16 frames per second with a resolution of 768√ó1360 pixels. The model effectively translates text prompts into coherent video narratives, addressing common limitations in previous video generation systems.\nThe model uses three key innovations:\nA 3D Variational Autoencoder (VAE) that compresses videos along both spatial and temporal dimensions, improving compression efficiency and video quality An expert transformer with adaptive LayerNorm that enhances text-to-video alignment through deeper fusion between modalities Progressive training and multi-resolution frame pack techniques that enable the creation of longer, coherent videos with significant motion elements CogVideoX also benefits from an effective text-to-video data processing pipeline with various preprocessing strategies and a specialized video captioning method, contributing to higher generation quality and better semantic alignment. The model‚Äôs weights are publicly available, making it accessible for implementation in various business applications, such as product demonstrations and marketing content. The following diagram shows the architecture of the model. Prompt enhancement To improve the quality of video generation, the solution provides an option to enhance user-provided prompts. This is done by instructing a large language model (LLM), in this case Anthropic‚Äôs Claude, to take a user‚Äôs initial prompt and expand upon it with additional details, creating a more comprehensive description for video creation. The prompt consists of three parts:\nRole section ‚Äì Defines the AI‚Äôs purpose in enhancing prompts for video generation Task section ‚Äì Specifies the instructions needed to be performed with the original prompt Prompt section ‚Äì Where the user‚Äôs original input is inserted By adding more descriptive elements to the original prompt, this system aims to provide richer, more detailed instructions to video generation models, potentially resulting in more accurate and visually appealing video outputs. We use the following prompt template for this solution: \u0026lt;Role\u0026gt; Your role is to enhance the user prompt that is given to you by providing additional details to the prompt. The end goal is to covert the user prompt into a short video clip, so it is necessary to provide as much information you can. \u0026lt;/Role\u0026gt; \u0026lt;Task\u0026gt; You must add details to the user prompt in order to enhance it for video generation. You must provide a 1 paragraph response. No more and no less. Only include the enhanced prompt in your response. Do not include anything else. \u0026lt;/Task\u0026gt; \u0026lt;Prompt\u0026gt; {prompt} \u0026lt;/Prompt\u0026gt; Prerequisites Before you deploy the solution, make sure you have the following prerequisites:\nThe AWS CDK Toolkit ‚Äì Install the AWS CDK Toolkit globally using npm: npm install -g aws-cdk This provides the core functionality for deploying infrastructure as code to AWS. Docker Desktop ‚Äì This is required for local development and testing. It makes sure container images can be built and tested locally before deployment. The AWS CLI ‚Äì The AWS Command Line Interface (AWS CLI) must be installed and configured with appropriate credentials. This requires an AWS account with necessary permissions. Configure the AWS CLI using aws configure with your access key and secret. Python Environment ‚Äì You must have Python 3.11+ installed on your system. We recommend using a virtual environment for isolation. This is required for both the AWS CDK infrastructure and Streamlit application. Active AWS account ‚Äì You will need to raise a service quota request for SageMaker to ml.g5.4xlarge for processing jobs. Deploy the solution This solution has been tested in the us-east-1 AWS Region. Complete the following steps to deploy:\nCreate and activate a virtual environment: python -m venv .` venv source .venv/bin/activate Install infrastructure dependencies: cd infrastructure pip install -r requirements.txt Bootstrap the AWS CDK (if not already done in your AWS account): cdk bootstrap Deploy the infrastructure: cdk deploy -c allowed_ips=\u0026#39;[\u0026#34;\u0026#39;$(curl -s ifconfig.me)\u0026#39;/32\u0026#34;]\u0026#39; To access the Streamlit UI, choose the link for StreamlitURL in the AWS CDK output logs after deployment is successful. The following screenshot shows the Streamlit UI accessible through the URL.\nBasic video generation Complete the following steps to generate a video:\nInput your natural language prompt into the text box at the top of the page. Copy this prompt to the text box at the bottom. Choose Generate Video to create a video using this basic prompt. The following is the output from the simple prompt ‚ÄúA bee on a flower.‚Äù Enhanced video generation For higher-quality results, complete the following steps:\nEnter your initial prompt in the top text box. Choose Enhance Prompt to send your prompt to Amazon Bedrock. Wait for Amazon Bedrock to expand your prompt into a more descriptive version. Review the enhanced prompt that appears in the lower text box. Edit the prompt further if desired. Choose Generate Video to initiate the processing job with CogVideoX. When processing is complete, your video will appear on the page with a download option.The following is an example of an enhanced prompt and output:\n\u0026#34;\u0026#34;\u0026#34; A vibrant yellow and black honeybee gracefully lands on a large, blooming sunflower in a lush garden on a warm summer day. The bee\u0026#39;s fuzzy body and delicate wings are clearly visible as it moves methodically across the flower\u0026#39;s golden petals, collecting pollen. Sunlight filters through the petals, creating a soft, warm glow around the scene. The bee\u0026#39;s legs are coated in pollen as it works diligently, its antennae twitching occasionally. In the background, other colorful flowers sway gently in a light breeze, while the soft buzzing of nearby bees can be heard \u0026#34;\u0026#34;\u0026#34; Add an image to your prompt If you want to include an image with your text prompt, complete the following steps:\n1.Complete the text prompt and optional enhancement steps. 2. Choose Include an Image. 3. Upload the photo you want to use. 4. With both text and image now prepared, choose Generate Video to start the processing job.\nThe following is an example of the previous enhanced prompt with an included image.\nTo view more samples, check out the CogVideoX gallery.\nClean up To avoid incurring ongoing charges, clean up the resources you created as part of this post:\ncdk destroy\nConsiderations Although our current architecture serves as an effective proof of concept, several enhancements are recommended for a production environment. Considerations include implementing an API Gateway with AWS Lambda backed REST endpoints for improved interface and authentication, introducing a queue-based architecture using Amazon Simple Queue Service (Amazon SQS) for better job management and reliability, and enhancing error handling and monitoring capabilities.\nConclusion Video generation technology has emerged as a transformative force in digital content creation, as demonstrated by our comprehensive AWS-based solution using the CogVideoX model. By combining powerful AWS services like Fargate, SageMaker, and Amazon Bedrock with an innovative prompt enhancement system, we‚Äôve created a scalable and secure pipeline capable of producing high-quality video clips. The architecture‚Äôs ability to handle both text-to-video and image-to-video generation, coupled with its user-friendly Streamlit interface, makes it an invaluable tool for businesses across sectors‚Äîfrom ecommerce product demonstrations to personalized marketing campaigns. As showcased in our sample videos, the technology delivers impressive results that open new avenues for creative expression and efficient content production at scale. This solution represents not just a technological advancement, but a glimpse into the future of visual storytelling and digital communication.\nTo learn more about CogVideoX, refer to CogVideoX on Hugging Face. Try out the solution for yourself, and share your feedback in the comments.\nAbout the Authors Nick Bisois a Machine Learning Engineer at AWS Professional Services. He solves complex organizational and technical challenges using data science and engineering. In addition, he builds and deploys AI/ML models on the AWS Cloud. His passion extends to his proclivity for travel and diverse cultural experiences.\nNatasha Tchir is a Cloud Consultant at the Generative AI Innovation Center, specializing in machine learning. With a strong background in ML, she now focuses on the development of generative AI proof-of-concept solutions, driving innovation and applied research within the GenAIIC.\nKatherine Feng is a Cloud Consultant at AWS Professional Services within the Data and ML team. She has extensive experience building full-stack applications for AI/ML use cases and LLM-driven solutions.\nJinzhao Feng is a Machine Learning Engineer at AWS Professional Services. He focuses on architecting and implementing large-scale generative AI and classic ML pipeline solutions. He is specialized in FMOps, LLMOps, and distributed training.\n"},{"uri":"https://github.com/nhutruong47/report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Deploy AWS Backup to automate data protection processes Explore AWS Storage Gateway for hybrid cloud storage Get started with Amazon S3 fundamentals and static website hosting Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Deploy AWS Backup for the system + Overview of AWS Backup and Amazon SNS + Deploy the infrastructure + Create backup plans for AWS resources 29/09/2025 29/09/2025 https://000013.awsstudygroup.com/ 2 - Finalize AWS Backup setup + Configure SNS notifications + Test restore procedures + Clean up resources 30/09/2025 30/09/2025 https://000013.awsstudygroup.com/ 3 - File Storage Gateway workshop + Preparation and setup + Create the Storage Gateway + Create file shares + Mount shares on on-premises machine + Resource cleanup 01/10/2025 01/10/2025 https://000024.awsstudygroup.com/ 4 - Amazon S3 basics (Part 1) + Introduction to Amazon S3 + Preparation and setup + Enable static website hosting + Configure public access block and object permissions + Test the website 02/10/2025 02/10/2025 https://000057.awsstudygroup.com/ 5 - Amazon S3 advanced topics (Part 2) + Speed up static website with CloudFront + Enable bucket versioning + Move objects and lifecycle policies + Configure cross-region replication + Notes \u0026amp; best practices 03/10/2025 03/10/2025 https://000057.awsstudygroup.com/ üèÜ Week 4 Achievements Understand AWS Backup service\nUnderstand AWS Backup as a centralized data-protection solution Created automated backup plans for multiple AWS resources Configured backup policies for EBS, RDS, DynamoDB, and EFS Configured SNS notifications for backup events Successfully tested backup and restore procedures Amazon S3 fundamentals and static website hosting\nLearned core Amazon S3 object storage concepts Configured static website hosting on S3 Set up public access blocks and adjusted object permissions Implemented CloudFront to accelerate the site Studied S3 versioning and object lifecycle management Advanced S3 features\nConfigured bucket versioning for better data protection Implemented object transitions and lifecycle policies Set up cross-region replication for disaster recovery Applied S3 best practices and security recommendations Understand S3 storage classes and cost-optimization strategies "},{"uri":"https://github.com/nhutruong47/report/5-workshop/5.4-s3-onprem/","title":"Access S3 from on-premises","tags":[],"description":"","content":"Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "},{"uri":"https://github.com/nhutruong47/report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nIn this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3‚Ä¶, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event‚Äôs content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in three important events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AI/ML/GenAI on AWS\nDate \u0026amp; Time: 09:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nContent Description: Comprehensive workshop on AI/ML/GenAI on AWS, including Amazon SageMaker, Foundation Models on Bedrock, Prompt Engineering, RAG, and building GenAI chatbots.\nValue Gained: Deep understanding of AWS AI/ML ecosystem, practical skills with SageMaker and Bedrock, ability to apply AI/GenAI to real-world projects.\nEvent 2 Event Name: AWS Cloud Mastery Series #2 - DevOps on AWS\nDate \u0026amp; Time: 09:00, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nContent Description: Event focused on DevOps practices on AWS, including CI/CD pipelines, Infrastructure as Code, Container services, and Monitoring \u0026amp; Observability.\nValue Gained: Mastery of DevOps culture and implementation on AWS, proficiency in CI/CD with CodeCommit/CodeBuild/CodeDeploy/CodePipeline, deep understanding of IaC and container orchestration.\nEvent 3 Event Name: AWS Cloud Mastery Series #3 - Well-Architected Security Pillar\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: 26th Floor, Bitexco Financial Tower, 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nContent Description: In-depth workshop on the 5 security pillars of AWS Well-Architected Framework: Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response.\nValue Gained: Comprehensive understanding of cloud security architecture, mastery of modern IAM practices with zero-trust principles, skills to implement comprehensive monitoring and automated incident response.\n"},{"uri":"https://github.com/nhutruong47/report/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"On-premises DNS Simulation","tags":[],"description":"","content":"AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"},{"uri":"https://github.com/nhutruong47/report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Master AWS Storage Services and Amazon S3 comprehensive features Learn AWS Backup and VM Import/Export strategies Practice Storage Gateway for hybrid cloud storage solutions Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Module 04-01: D·ªãch V·ª• L∆∞u Tr·ªØ Tr√™n AWS - Module 04-02: Amazon Simple Storage Service (S3) - Access Point - Storage Class - Module 04-03: S3 Static Website \u0026amp; CORS - Control Access - Object Key \u0026amp; Performance - Glacier - Module 04-04: Snow Family - Storage Gateway - Backup - Lab 13: AWS Backup Implementation + Create S3 Bucket for backup storage + Deploy backup infrastructure + Create comprehensive Backup Plan + Set up notification systems with SNS + Test restore operations + Clean up backup resources 06/10/2025 06/10/2025 https://000013.awsstudygroup.com/ 2 - Lab 14: VM Import/Export Complete Workflow + Set up VMware Workstation environment + Export Virtual Machine from on-premises + Upload virtual machine to AWS S3 + Import virtual machine to AWS EC2 + Deploy Instance from custom AMI + Configure S3 bucket ACL for VM storage + Export virtual machine from EC2 Instance + Resource cleanup on AWS Cloud 07/10/2025 07/10/2025 https://000014.awsstudygroup.com/ 3 - Practice Day: Hands-on Review + Practice AWS Storage Services configuration + Review S3 advanced features implementation + Test AWS Backup scenarios + Validate VM Import/Export workflow + Troubleshooting common issues 08/10/2025 08/10/2025 4 - Lab 24: Storage Gateway Implementation + Create Storage Gateway + Create File Shares configuration + Mount File shares on on-premises machine + Test hybrid storage functionality - Practice Session: + Storage Gateway troubleshooting + Performance optimization + Best practices review 09/10/2025 09/10/2025 https://000024.awsstudygroup.com/ 5 - Comprehensive Practice \u0026amp; Review + End-to-end AWS Storage services workflow + AWS Backup automation testing + VM migration scenario practice + Storage Gateway integration testing + Week 5 knowledge consolidation 10/10/2025 10/10/2025 üèÜ Week 5 Achievements Learned AWS Storage Services\nUnderstood AWS storage service types and categories Learned S3 Access Points and Storage Classes Configured S3 Static Website hosting and CORS Explored S3 performance optimization and Glacier Studied AWS Snow Family and Storage Gateway Completed AWS Backup Implementation\nCreated S3 bucket for backup storage Deployed backup infrastructure successfully Set up backup plans and SNS notifications Tested data restore operations Cleaned up backup resources properly Mastered VM Import/Export Process\nSet up VMware Workstation environment Exported and imported virtual machines Deployed instances from custom AMI Configured S3 bucket ACL for VM storage Completed VM migration with cleanup Implemented Storage Gateway\nCreated and configured Storage Gateway Set up File Shares successfully Mounted file shares on on-premises machines Tested hybrid storage functionality Gained Hands-on Experience\nPracticed AWS Storage services configuration Performed backup and restore testing Completed VM migration scenarios Learned troubleshooting techniques "},{"uri":"https://github.com/nhutruong47/report/5-workshop/5.5-policy/","title":"VPC Endpoint Policies","tags":[],"description":"","content":"When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"},{"uri":"https://github.com/nhutruong47/report/5-workshop/","title":"Workshop","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "},{"uri":"https://github.com/nhutruong47/report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Gain an understanding of Amazon FSx for Windows File Server architecture and common use cases Deploy and configure Multi‚ÄëAZ FSx file systems (SSD and HDD) Practice performance testing, monitoring, and performance optimization for FSx Tasks for this week: Day Task Start Date Completion Date Reference Material 1 - Module 05-01: Amazon FSx for Windows File Server Overview + FSx architecture and integration with Active Directory + Typical use cases and basic operations 13/10/2025 13/10/2025 https://000025.awsstudygroup.com/ 2 - Lab 25 (Part 1): Amazon FSx for Windows File Server 2.1 Create environment 2.2 Create an SSD Multi‚ÄëAZ file system 2.3 Create an HDD Multi‚ÄëAZ file system 14/10/2025 14/10/2025 https://000025.awsstudygroup.com/ 3 - Lab 25 (Part 2): File shares \u0026amp; performance ‚Ä¢ Create file shares ‚Ä¢ Test performance ‚Ä¢ Monitor performance 15/10/2025 15/10/2025 https://000025.awsstudygroup.com/ 4 - Lab 25 (Part 3): Data management features ‚Ä¢ Enable data deduplication ‚Ä¢ Enable shadow copies ‚Ä¢ Manage user sessions and open files ‚Ä¢ Enable user storage quotas ‚Ä¢ Scale throughput capacity ‚Ä¢ Scale storage capacity ‚Ä¢ Delete environment 16/10/2025 16/10/2025 https://000025.awsstudygroup.com/ 5 - Practice \u0026amp; Review: + Go through Lab 25 steps again (create, share, test, monitor) + Repeat key tasks on FSx to reinforce workflow + Record common issues and troubleshooting notes 17/10/2025 17/10/2025 https://000025.awsstudygroup.com/ üèÜ Week 6 Achievements Basic understanding of Amazon FSx for Windows File Server\nGained an understanding of FSx for storing files in Windows environments Confirmed FSx integration with Active Directory on AWS FSx Deployment\nBuilt the lab environment Deployed SSD and HDD Multi‚ÄëAZ file systems File Shares\nCreated file shares on FSx Connected from a Windows client and performed read/write tests Performed basic performance checks Data Features\nEnabled data deduplication and shadow copies Managed user sessions and open files Applied basic user storage quotas "},{"uri":"https://github.com/nhutruong47/report/5-workshop/5.6-cleanup/","title":"Clean up","tags":[],"description":"","content":"Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "},{"uri":"https://github.com/nhutruong47/report/6-self-evaluation/","title":"Self-evaluation","tags":[],"description":"","content":"Throughout my internship at AMAZON WEB SERVICES VIETNAM COMPANY LIMITED from 09/09/2025 to 13/12/2025, I had the valuable opportunity to learn, develop, and apply theoretical knowledge acquired at university to a real professional work environment.\nDuring the internship, I actively participated in developing an intelligent AI chatbot project using advanced AWS services such as Amazon Bedrock, Lambda Functions, S3, and API Gateway, thereby not only enhancing my skills in proficient use of AWS cloud services, developing Python and JavaScript programming skills, improving effective teamwork capabilities, and learning to apply AI/ML to real-world solutions.\nRegarding work attitude, I always maintained a positive mindset, proactively completed assigned tasks with excellence, strictly adhered to company regulations, and was proactive in exchanging ideas and learning from colleagues as well as mentors to continuously improve work efficiency and output quality.\nTo reflect the most objective and honest assessment of my internship process, I would like to evaluate myself based on the following professional criteria:\nNo. Criteria Description Good Fair Average 1 Knowledge and Technical Skills Industry understanding, practical knowledge application, tool usage skills, work quality ‚òê ‚úÖ ‚òê 2 Learning Ability Absorbing new knowledge, learning quickly ‚òê ‚úÖ ‚òê 3 Initiative Self-research, taking on tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of Responsibility Completing work on time, ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Following schedules, regulations, work procedures ‚úÖ ‚òê ‚òê 6 Growth Mindset Willingness to receive feedback and improve oneself ‚òê ‚úÖ ‚òê 7 Communication Presenting ideas clearly, reporting work effectively ‚òê ‚úÖ ‚òê 8 Team Collaboration Working effectively with colleagues, participating in teams ‚úÖ ‚òê ‚òê 9 Professional Conduct Respecting colleagues, partners, work environment ‚úÖ ‚òê ‚òê 10 Problem-solving Thinking Identifying issues, proposing solutions, creativity ‚òê ‚úÖ ‚òê 11 Contribution to Projects/Organization Work effectiveness, improvement initiatives, team recognition ‚úÖ ‚òê ‚òê 12 Overall General assessment of the entire internship process ‚úÖ ‚òê ‚òê Areas for Improvement Enhance discipline, strictly comply with company regulations or any organization\u0026rsquo;s policies Improve problem-solving thinking approaches Learn better communication skills in daily interactions and work situations, handling various scenarios "},{"uri":"https://github.com/nhutruong47/report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Understand Amazon S3 fundamentals and main use cases Review Amazon S3 fundamentals and common use cases Create and configure S3 buckets for hosting static websites Practice access control, CloudFront integration, and enabling versioning Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Module: Starting with Amazon S3 + Learn about Amazon S3 basics and concepts (buckets, objects, regions) 20/10/2025 20/10/2025 https://000057.awsstudygroup.com/ 2 - Lab (Part 1): Create and prepare S3 bucket 2. Create S3 bucket 2.1 Download source code to the device (load data) 3. Enable static website feature 21/10/2025 21/10/2025 https://000057.awsstudygroup.com/ 3 - Lab (Part 2): Public access and testing 4. Configure public access block 5. Configure public objects 6. Test static website 22/10/2025 22/10/2025 https://000057.awsstudygroup.com/ 4 - Lab (Part 3): CloudFront, versioning and replication 7. Accelerate static websites with CloudFront 7.1 Block all public access 7.2 Configure Amazon CloudFront 7.3 Test Amazon CloudFront 8. Bucket versioning 9. Move objects 10. Replicate objects across multiple Regions 11. Clean up resources 12. Notes \u0026amp; Best Practices 23/10/2025 23/10/2025 https://000057.awsstudygroup.com/ 5 - Practice \u0026amp; Review: + Repeat full S3 static website workflow (create bucket, upload code, host website) + Practice configuring public access, CloudFront, and versioning again + Review notes and best practices from the lab 24/10/2025 24/10/2025 https://000057.awsstudygroup.com/ üèÜ Week 7 Achievements Amazon S3 Basics\nClarified common uses of Amazon S3 Reviewed key concepts: buckets, objects, regions, and hosting static website content Created and Configured S3 Bucket\nCreated an S3 bucket for hosting a static website Downloaded the source code locally and uploaded content to S3 Enabled static website hosting on the bucket Managed Public Access and Testing\nAdjusted S3 public access block settings Made required objects publicly accessible Accessed and tested the static website URL Integrated with Amazon CloudFront\nRestricted direct public access to the S3 bucket Configured a CloudFront distribution in front of the S3 website Tested the CloudFront distribution URL for improved performance and security Used Versioning and Replication\nEnabled bucket versioning to track object versions Moved objects between buckets/folders as needed Set up cross-Region replication for objects "},{"uri":"https://github.com/nhutruong47/report/7-feedback/","title":"Share and Contribute Feedback","tags":[],"description":"","content":" Here, I would like to share my most sincere feelings about the internship experience at AWS Vietnam, hoping this feedback can help the FCJ team and future generations of interns have better experiences.\nGeneral Assessment 1. Working Environment The working environment at AWS Vietnam is truly impressive and professional. The modern office at Bitexco Financial Tower creates a sense of pride when working here. FCJ team members are always ready to assist when I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. The atmosphere is dynamic but not stressful, allowing me to maximize my abilities.\n2. Mentor / Admin Team Support The mentor provided very detailed guidance, explained clearly when I didn\u0026rsquo;t understand, and always encouraged me to ask questions. The admin team supported procedures, documentation, and created favorable conditions for my work. I highly appreciate that the mentor allowed me to try and solve problems by myself instead of just providing the answers.\n3. Suitability of Work to Academic Major The tasks I was assigned were suitable for the knowledge I learned at university, while also broadening my exposure to new areas I had not yet accessed. As a result, I both consolidated my foundational knowledge and learned practical skills.\n4. Learning Opportunities \u0026amp; Skill Development This is the point I appreciate the most. I not only learned technical skills but also developed professional skills, teamwork skills, and professional communication in a corporate environment. Mr. Hoang Van Kha also shared a lot of practical experience, helping me better orient my career path.\n5. Culture \u0026amp; Team Spirit The company culture is very positive: everyone respects each other, works seriously but with joy. When there\u0026rsquo;s an urgent project, everyone works together, providing support regardless of position. This made me feel like I was a part of the collective, even as just an intern.\n6. Intern Policies / Benefits The company provided internship allowance and offered flexibility in working hours when necessary. FCJ members were always ready to answer student questions, even outside of working hours. Furthermore, being able to participate in training workshops was a major plus.\nOther Questions What did you enjoy most during your internship? - The thing that made me most satisfied and fond of the experience was the supportive spirit and professional discipline of all the seniors in the FCJ team. Whenever I faced challenges, I received dedicated guidance, which helped me feel more confident and stable in my work.\nWhat do you think the company needs to improve for subsequent interns? - To help future interns feel more connected and close to each other, as well as to the company culture, I hope the company can enhance networking activities. Specifically, I propose: Organizing meetups for the members within our team so everyone has the opportunity to socialize, get acquainted, and work together more easily. Additionally, if there were more In-depth Training sessions on specific professional areas, it would be very beneficial for our career development orientation.\nIf recommending to friends, would you advise them to intern here? Why? - Absolutely YES! I would recommend it immediately. This is an ideal working environment: professional yet incredibly welcoming, with dedicated mentors, and clear work, suitable for developing comprehensive skills, from specialized knowledge to professional conduct.\nProposals \u0026amp; Wishes Do you have any proposals to improve the internship experience? - I propose focusing on community building and networking, specifically:\n* Increasing in-person meetups at the office: As I suggested, small team gatherings would help everyone feel closer, more bonded, and gain a clearer understanding of the company culture and the work itself.\n* Organizing additional experience sharing sessions or internal knowledge sharing among different departments to broaden the interns\u0026rsquo; knowledge and perspective.\nWould you like to continue this program in the future? - I am very eager to continue participating in this program or other company projects in the future, as I truly found a positive working environment and a great opportunity for self-development here.\nOther Feedback (Free sharing): - I would like to express my sincere thanks to the Company and the FCJ Team for creating such wonderful conditions and supporting me wholeheartedly. I hope the internship program will continue to improve, and that my feedback will bring even more value to the future generations of interns!\n"},{"uri":"https://github.com/nhutruong47/report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Learn to use tags to organize and manage AWS resources Practice creating Resource Groups and filtering resources using tags Manage EC2 access using resource tags and IAM policies Understand IAM permission boundaries to limit user permissions Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Lab 27: Manage Resources Using Tags and Resource Groups 2. Using Tags 2.1 Use tags on Console - 2.1.1 Create EC2 instance with tag - 2.1.2 Manage tags on AWS resources - 2.1.3 Filter resources by tag 2.2 Use tags with CLI 3. Create a Resource Group 4. Clean up resources 03/11/2025 03/11/2025 https://000027.awsstudygroup.com/ 2 - Lab 28: Manage access to EC2 services with resource tags through IAM services 1. Introduction 2. Preparation - 2.1 Create IAM user 3. Create IAM Policy 4. Create IAM Role 5. Check Policy: - 5.1 Switch Roles - 5.2 Check IAM Policy - 5.2.1 Access EC2 console in Tokyo Region - 5.2.2 Access EC2 console in North Virginia Region - 5.2.3 Create EC2 instance with/without required tags - 5.2.4 Edit resource tag on EC2 instance - 5.2.5 Policy check results 6. Clean up resources 04/11/2025 04/11/2025 https://000028.awsstudygroup.com/ 3 - Practice: Review Lab 27 \u0026amp; Lab 28 + Repeat using tags on Console and CLI + Practice filtering resources and using Resource Groups + Practice tag-based access control for EC2 with IAM policies + Note simple best practices for tags and access control 05/11/2025 05/11/2025 https://000027.awsstudygroup.com/, https://000028.awsstudygroup.com/ 4 - Lab 30: Limitation of user rights with IAM Permission Boundary 1. Introduction 2. Preparation 3. Create restriction policy (permission boundary) 4. Create IAM limited user 5. Test IAM user limits 6. Clean up resources 06/11/2025 06/11/2025 https://000030.awsstudygroup.com/ 5 - Practice: Review Tag, IAM Policy, and Permission Boundary + Review how tags and Resource Groups help manage resources + Review tag-based access control to EC2 from Lab 28 + Review IAM permission boundary from Lab 30 + Summarize what was learned in Week 9 07/11/2025 07/11/2025 https://000027.awsstudygroup.com/, https://000028.awsstudygroup.com/, https://000030.awsstudygroup.com/ üèÜ Week 9 Achievements Tags and Resource Management\nUsed tags in the AWS Console to label EC2 and other resources Filtered resources by tags for easier discovery Used the AWS CLI to manage tags and create Resource Groups Tag-based Access Control for EC2\nCreated IAM user, policy, and role for tag-based access control Tested EC2 console access across Regions (Tokyo, N. Virginia) Tested creating EC2 instances with and without required tags Edited resource tags to observe policy allow/deny behavior IAM Permission Boundaries\nCreated a restriction policy to use as a permission boundary Created an IAM user with limited permissions Tested the user\u0026rsquo;s allowed and disallowed actions in the AWS console Practice and Cleanup\nPracticed using tags, Resource Groups, and IAM policies multiple times Cleaned up lab resources after finishing Gained a clearer understanding of how tags and IAM work together to control access "},{"uri":"https://github.com/nhutruong47/report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Review the AWS shared responsibility model and core security and IAM services Learn and practice using AWS Security Hub and its recommended standards Explore ways to optimize EC2 costs using AWS Lambda automation Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Module 05: AWS Security and IAM Services + Module 05-01: Shared Responsibility Model + Module 05-02: AWS Identity and Access Management (IAM) + Module 05-03: Amazon Cognito + Module 05-04: AWS Organizations + Module 05-05: AWS Identity Center + Module 05-06: AWS Key Management Service (KMS) + Module 05-07: AWS Security Hub + Module 05-08: Hands-on and additional research 27/10/2025 27/10/2025 https://000018.awsstudygroup.com/ 2 - Lab 18 (Part 1): Introduction to AWS Security Hub + Review Security Standards and AWS Foundational Security Best Practices + 2. Enable Security Hub 28/10/2025 28/10/2025 https://000018.awsstudygroup.com/ 3 - Lab 18 (Part 2): Review Security Hub findings + 3. Check scores for each set of criteria + Review findings and understand security recommendations + 4. Clean up resources 29/10/2025 29/10/2025 https://000018.awsstudygroup.com/ 4 - Lab 22: Automating EC2 cost optimization with Lambda + 1. Understand how Lambda improves cost efficiency + 2. Preparation: - 2.1 Create VPC - 2.2 Create Security Group - 2.3 Create EC2 instance - 2.4 Configure Slack incoming webhooks + 3. Create tags for EC2 instances + 4. Create IAM Role for Lambda + 5. Create Lambda functions: - 5.1 Function to stop instances - 5.2 Function to start instances + 6. Verify automation results + 7. Clean up resources 30/10/2025 30/10/2025 https://000022.awsstudygroup.com/ 5 - Practice \u0026amp; Review: + Review all Module 05 security and IAM concepts + Practice again enabling Security Hub and reading basic findings + Review Lambda functions for EC2 cost optimization (start/stop instances) + Note simple best practices for security and cost management 31/10/2025 31/10/2025 https://000018.awsstudygroup.com/, https://000022.awsstudygroup.com/ üèÜ Week 8 Achievements AWS Security and IAM Basics\nReviewed the AWS shared responsibility model and core IAM concepts Covered core concepts: IAM, Cognito, AWS Organizations, Identity Center, and KMS Used AWS Security Hub\nEnabled AWS Security Hub in the account Reviewed security standards and AWS Foundational Security Best Practices Checked security scores and findings for various checks Cleaned up lab resources used for Security Hub Optimized EC2 Costs with Lambda\nCreated VPC, security group, and EC2 instance for the lab Configured Slack incoming webhooks for notifications Tagged instances for automation Created IAM Role and Lambda functions to stop and start instances Tested the Lambda automation and verified instance control Cleaned up EC2 and Lambda resources after the lab "},{"uri":"https://github.com/nhutruong47/report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Practice encrypting data at rest with AWS KMS integrated with S3 and analyze logs using CloudTrail/Athena Review IAM roles, condition keys, and access control patterns Practice using IAM roles for applications (EC2 ‚Üí S3) Learn core AWS database services: Amazon RDS, Aurora, Redshift, and ElastiCache Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Lab 33: Encrypt at rest with AWS KMS 1. Introduction 2. Preparation steps: - 2.1 Create policy and role - 2.2 Create group and user 3. Create AWS Key Management Service (KMS) key 4. Create Amazon S3: - 4.1 Create bucket - 4.2 Upload data to S3 5. Create AWS CloudTrail and Amazon Athena: - 5.1 Create CloudTrail - 5.2 Logging to CloudTrail - 5.3 Create Amazon Athena - 5.4 Retrieve data with Athena 6. Test and share encrypted data on S3 7. Resource cleanup 10/11/2025 10/11/2025 https://000033.awsstudygroup.com/ 2 - Lab 44: IAM Roles \u0026amp; Conditions 1. Introduction to IAM concepts - 1.1 Request to AWS service - 1.2 Authenticate requests - 1.3 Assume role workflow 2. Create IAM Group 3. Create IAM Users: - 3.1 Create IAM users - 3.2 Verify permissions 4. Configure Role Conditions: - 4.1 Create Admin IAM Role - 4.2 Configure switch role - 4.3 Restrict role access: ‚Ä¢ 4.3.1 Limit switch role by IP ‚Ä¢ 4.3.2 Limit switch role by time 5. Clean up resources 11/11/2025 11/11/2025 https://000044.awsstudygroup.com/ 3 - Practice: Review Lab 33 \u0026amp; Lab 44 + Practice creating and using KMS keys to encrypt S3 data + Review CloudTrail and Athena queries for KMS/S3 activity + Practice IAM roles with conditions (IP, time) and switching roles + Take brief notes on KMS, IAM roles, and condition keys 12/11/2025 12/11/2025 https://000033.awsstudygroup.com/, https://000044.awsstudygroup.com/ 4 - Lab 48: Granting application access to AWS services using IAM roles 1. Preparation: - 1.1 Create EC2 instance - 1.2 Create S3 bucket 2. Use access key: - 2.1 Generate IAM user and access key - 2.2 Use access key to access S3 from the application 3. IAM role on EC2: - 3.1 Create IAM role - 3.2 Use IAM role on EC2 instead of access keys 4. Clean up resources 13/11/2025 13/11/2025 https://000048.awsstudygroup.com/ 5 - Module 06: AWS Database Services + Module 06-01: Database Concepts Review + Module 06-02: Amazon RDS \u0026amp; Amazon Aurora + Module 06-03: Amazon Redshift \u0026amp; ElastiCache + Note key differences between relational, data warehouse, and in-memory cache services 14/11/2025 14/11/2025 üèÜ Week 10 Achievements KMS and Encryption at Rest\nCreated and managed AWS KMS keys Encrypted data in S3 and verified access via CloudTrail and Athena Tested sharing encrypted S3 data and cleaned up lab resources IAM Roles and Conditions\nReviewed IAM basics: requests, authentication, and the assume-role workflow Created IAM users, groups, and admin roles Applied role conditions to restrict access by IP address and time Application Access with IAM Roles\nCreated an EC2 instance and S3 bucket for the lab Used IAM user access keys to access S3, then replaced them with IAM roles Verified EC2 accessed S3 securely using roles instead of long-term keys Database Service Overview\nReviewed core database concepts on AWS Covered basics of Amazon RDS and Aurora for relational databases Observed how Redshift and ElastiCache support analytics and caching workloads "},{"uri":"https://github.com/nhutruong47/report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Practice building and managing relational databases with Amazon RDS Explore database migration tools and scenarios (Lab 43) Build a simple data lake on AWS using S3, Glue, Athena, and QuickSight Learn to create and operate Amazon DynamoDB, including backups and migrations Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Lab 05: Amazon Relational Database Service (Amazon RDS) 1. Introduction 2. Prerequisite steps: - 2.1 Create a VPC - 2.2 Create EC2 security group - 2.3 Create RDS security group - 2.4 Create DB subnet group 3. Create EC2 instance 4. Create RDS database instance 5. Application deployment 6. Backup and restore 7. Clean up resources 17/11/2025 17/11/2025 https://000005.awsstudygroup.com/ 2 - Lab 43: (DB migration \u0026amp; tools) 01. EC2 Connect RDP Client 02. EC2 Connect Fleet Manager 03. SQLSrv Src Config 04. Oracle connect SrcDB 05. Oracle config SrcDB 06. Drop Constraint 07. MSSQL to Aurora MySQL target config 08. MSSQL to Aurora MySQL create project 09. MSSQL to Aurora MySQL schema conversion 10. Oracle to MySQL schema conversion (part 1) 11. Create migration task and endpoint 12. Inspect S3 13. Create serverless migration 14. Create event notifications 15. Check logs 16. Troubleshoot memory pressure scenario 17. Troubleshoot table error scenario 18/11/2025 18/11/2025 Lab 43 3 - Lab 35: Data Lake on AWS 1. Introduce Data Lake (Big Data concept) 2. Preparation steps 3. Data collection and storage: - 3.1 Create S3 bucket - 3.2 Create Kinesis Firehose delivery stream (or similar) - 3.3 Create sample data 4. Create data catalog: - 4.1 Create AWS Glue crawler - 4.2 Check data/catalog 5. Data transformation 6. Analysis and visualization: - 6.1 Analyze with Athena - 6.2 Visualize with QuickSight 7. Clean up resources 19/11/2025 19/11/2025 https://000035.awsstudygroup.com/ 4 - Practice: Review Lab 05 \u0026amp; Lab 35 + Practice again creating RDS instances and connecting from EC2 + Review backup and restore steps for RDS + Practice creating S3 buckets, Glue crawlers, and running Athena queries + Write short notes comparing RDS vs data lake (S3 + Glue + Athena) 20/11/2025 20/11/2025 https://000005.awsstudygroup.com/, https://000035.awsstudygroup.com/ 5 - Lab 39: Learn to build and work with Amazon DynamoDB 1. LHOL: Hands-on lab for Amazon DynamoDB 2. Exploring DynamoDB 3. Exploring the DynamoDB Console 4. Backups 5. Cleanup 6. LADV: Advanced design patterns for Amazon DynamoDB 7. LMR: Building and deploying global serverless applications with DynamoDB 8. LEDA: Building a serverless event-driven architecture with DynamoDB 2025/11/21 2025/11/21 https://000039.awsstudygroup.com/ üèÜ Week 11 Achievements Amazon RDS basics\nCreated an RDS database and connected from an EC2 instance Practiced simple application deployment and backup/restore Database migration\nFamiliarized with migration tools (SQL Server/Oracle ‚Üí Aurora MySQL) Reviewed logs, inspected S3 data, and handled sample issues Data Lake on AWS\nCreated an S3 bucket, AWS Glue crawler, and data catalog Queried data with Athena and viewed basic reports in QuickSight Amazon DynamoDB\nCreated DynamoDB tables and loaded sample data Read and wrote data via CLI and Console; reviewed backup options "},{"uri":"https://github.com/nhutruong47/report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Practice building and inspecting cost-related data in AWS Get familiar with different ways to interact with AWS (CloudShell, Console, SDK) Use AWS Glue DataBrew and Cloud9 to prepare and transform data Build an end-to-end analytics pipeline with Glue, EMR, Athena, Kinesis Data Analytics, QuickSight, Lambda, and Redshift Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Lab 40: (Cost and usage data) 2.1 Preparing the database 2.2 Building a database 3.1 Data in the table 3.2 Cost 3.3 Tagging and cost allocation 3.4 Usage 3.5 Additional result query 4 Clean up resources 24/11/2025 24/11/2025 https://000040.awsstudygroup.com/ 2 - Lab 60 \u0026amp; Lab 70: AWS tools and data preparation Lab 60: CloudShell, Console, SDK Lab 70: Cloud9, dataset, S3, and AWS Glue DataBrew (profiling, clean \u0026amp; transform) 25/11/2025 25/11/2025 https://000060.awsstudygroup.com/, https://000070.awsstudygroup.com/ 3 - Practice: Review Lab 40 \u0026amp; Lab 70 + Practice checking cost/usage data and queries from Lab 40 + Practice using Cloud9 and S3 to prepare data + Practice creating and running basic DataBrew jobs for profiling and cleaning data + Write short notes about how these tools help with cost and data preparation 26/11/2025 26/11/2025 https://000040.awsstudygroup.com/, https://000070.awsstudygroup.com/ 4 - Lab 72: End-to-end analytics pipeline Preparatory steps, ingest \u0026amp; store, catalog data Transform with Glue (interactive \u0026amp; GUI), DataBrew, EMR Analysis with Athena and Kinesis Data Analytics Visualize in QuickSight, serve with Lambda, warehouse on Redshift 27/11/2025 27/11/2025 https://000072.awsstudygroup.com/ 5 - Lab 73 + Practice: Dashboards and review Build dashboard, improve dashboard, create interactive dashboard Practice reviewing pipeline parts from Lab 72 (Athena, Kinesis Data Analytics, QuickSight, Lambda, Redshift) Note simple best practices for building clear dashboards and reports 28/11/2025 28/11/2025 https://000073.awsstudygroup.com/, https://000072.awsstudygroup.com/ üèÜ Week 12 Achievements Cost and usage data basics\nBuilt a simple database to store cost and usage data Viewed table data, costs, tags, and usage information Ran additional queries and cleaned up lab resources AWS tools and data preparation\nUsed CloudShell, Console, and SDK to interact with AWS Created a Cloud9 environment and worked with datasets in S3 Used AWS Glue DataBrew for data profiling, cleaning, and transformation End-to-end analytics pipeline\nIngested and stored data, created a data catalog with Glue Transformed data using Glue (interactive, GUI), DataBrew, and EMR Analyzed data with Athena and Kinesis Data Analytics "},{"uri":"https://github.com/nhutruong47/report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://github.com/nhutruong47/report/tags/","title":"Tags","tags":[],"description":"","content":""}]