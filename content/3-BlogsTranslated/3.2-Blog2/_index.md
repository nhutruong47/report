---
title: "Blog 2"
date: "2025-07-10"
weight: 2
chapter: false
pre: " <b> 3.2. </b> "
---
# Democratize data for timely decisions with text-to-SQL at Parcel Perform

by Yudho Ahmad Diponegoro, Le Vy, and Jun Kai Loke | on July 09, 2025 | in [Amazon Athena](https://aws.amazon.com/athena/), [Amazon Bedrock](https://aws.amazon.com/bedrock/), [Amazon Bedrock Knowledge Bases](https://aws.amazon.com/bedrock/knowledge-bases/), [Business Intelligence](https://aws.amazon.com/business-intelligence/), [Customer Solutions](https://aws.amazon.com/solutions/), [Generative AI](https://aws.amazon.com/generative-ai/), [Intermediate (200)](https://aws.amazon.com/training/), [Supply Chain](https://aws.amazon.com/supply-chain/)

*This post is co-written with Le Vy from Parcel Perform.*
---
Access to accurate data is often the true differentiator between excellent decisions and timely decisions. This becomes even more critical for customer-facing decisions and actions. A modern AI deployed correctly can help your organization simplify data access to make accurate and timely decisions for customer-facing business teams, while minimizing the undifferentiated heavy lifting that your data team has to do. In this post, we share how [Parcel Perform](https://www.parcelperform.com/), a leading AI Delivery Experience platform for global ecommerce businesses, has implemented such a solution.

Accurate post-purchase deliveries tracking can be crucial for many ecommerce merchants. Parcel Perform provides an AI-driven, intelligent end-to-end data and delivery experience and software as a service (SaaS) system for ecommerce merchants. The system uses AWS services and state-of-the-art AI to process hundreds of millions of daily parcel delivery movement data and provide a unified tracking capability across couriers for the merchants, with emphasis on accuracy and simplicity.

The business team in Parcel Perform often needs access to data to answer questions related to merchants’ parcel deliveries, such as “Did we see a spike in delivery delays last week? If so, in which transit facilities were this observed, and what was the primary cause of the issue?” Previously, the data team had to manually form the query and run it to fetch the data. With the new generative AI-powered text-to-SQL capability in Parcel Perform, the business team can self-serve their data needs by using an AI assistant interface. In this post, we discuss how Parcel Perform incorporated generative AI, data storage, and data access through AWS services to make timely decisions. 

---

## Data analytics architecture

The solution starts with data ingestion, storage, and access. Parcel Perform adopts a data analytics architecture as shown in the following diagram.

![Data Analytics Architecture](/images/3-Blog/ML-18476-data-architecture.png)

One key data type in the Parcel Perform parcel monitoring application is the parcel event data, which can reach billions of rows. This includes the parcel’s shipment status change, location change, and much more. This day-to-day data from multiple business units lands in relational databases hosted on  [Amazon Relational Database Service](https://aws.amazon.com/rds/) (Amazon RDS).

Although relational databases are suitable for rapid data ingestion and consumption from the application, a separate analytics stack is needed to handle analytics in a scalable and performant way without disrupting the main application. These analytics needs include answering aggregation queries from questions like “How many parcels were delayed last week?”

Parcel Perform uses [Amazon Simple Storage Service](https://aws.amazon.com/s3/) (Amazon S3) with a query engine provided by [Amazon Athena](https://aws.amazon.com/athena/) to meet their analytics needs. With this approach, Parcel Perform benefits from cost-effective storage while still being able to run SQL queries as needed on the data through Athena, which is priced on usage.

Data in Amazon S3 is stored in [Apache Iceberg](https://iceberg.apache.org/) data format that allows data updates, which is useful in this case because the parcel events sometimes get updated. It also supports partitioning for better performance [Amazon S3 Tables](https://aws.amazon.com/s3/features/tables/), launched in late 2024, is a feature for managing Iceberg tables, and could also be an option for you.

Parcel Perform uses an [Apache Kafka](https://kafka.apache.org/) cluster managed by [Amazon Managed Streaming for Apache Kafka](https://aws.amazon.com/msk/) (Amazon MSK) as a stream to transfer data from source to S3 bucket. [Amazon MSK Connect](https://aws.amazon.com/msk/features/msk-connect/) with Debezium connector streams data using change data capture (CDC) from Amazon RDS to Amazon MSK.

[Apache Flink](https://flink.apache.org/), running on [Amazon Elastic Kubernetes Service](https://aws.amazon.com/eks/) (Amazon EKS), processes the data streams from Amazon MSK. It writes this data to the S3 bucket in Iceberg format, and updates the data schema in [AWS Glue Data Catalog](https://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html). This data schema allows Athena to query the correct data in the S3 bucket.

Now that you understand how data is ingested and stored, we'll look at how data is consumed through a data serving assistant using generative AI for business teams at Parcel Perform.

---

## Data-queryable AI agent

The users of the data serving AI agent at Parcel Perform are customer-facing business team members who regularly query parcel event data to answer questions from ecommerce merchants about deliveries and support them proactively. The following screenshot shows the AI assistant UI experience, powered by text-to-SQL with generative AI.

![AI Assistant UI](/images/3-Blog/ML-18476-ai-assistant-screenshot.png)

This functionality helped the Parcel Perform team and their customers save time, which we discuss later in this post. In the following section, we present the architecture that powers this feature.

---

## Text-to-SQL AI agent architecture

The data serving AI assistant architecture in Parcel Perform is shown in the following diagram.

![Text-to-SQL Architecture](/images/3-Blog/ML-18476-ai-assistant-architecture.png)

The AI assistant UI is supported by an application built with the [FastAPI](https://fastapi.tiangolo.com/) framework hosted on Amazon EKS. It is also fronted by an [Application Load Balancer](https://aws.amazon.com/elasticloadbalancing/application-load-balancer/) to allow for potential horizontal scalability.

The application uses [LangGraph](https://www.langchain.com/langgraph) to orchestrate the workflow of large language model (LLM) calls, tool usage, and memory checkpointing. The graph uses multiple tools, including tools from the [SQLDatabase Toolkit](https://python.langchain.com/docs/integrations/tools/sql_database/) to automatically retrieve data schema through Athena. The graph also uses an [Amazon Bedrock Knowledge Bases retriever](https://python.langchain.com/docs/integrations/retrievers/bedrock/) to retrieve business information from a knowledge base. Parcel Perform uses [Anthropic's Claude models in Amazon Bedrock](https://aws.amazon.com/bedrock/claude/) to generate SQL.

Although the function of Athena as a query engine to query the parcel event data on Amazon S3 is clear, Parcel Perform still needs a knowledge base. In this use case, the SQL generation performs better when the LLM has more business contextual information to help interpret database fields and translate logistics terminology into data representations. This is better illustrated with the following two examples:

1. Parcel Perform’s data lake operations use specific codes `c` for create and `u` for update. When analyzing data, Parcel Perform sometimes needs to focus only on initial creation records, where operation code is equal to `c`. Because this business logic might not be inherent in the training of LLMs in general, Parcel Perform explicitly defines this in their business context.

2. In logistics terminology, transit time has specific industry conventions. It’s measured in days, and same-day deliveries are recorded as `transit_time = 0`. Although this is intuitive for logistics professionals, an LLM might incorrectly interpret a request like “Get me all shipments with same-day delivery” by using`WHERE transit_time = 1` instead of `WHERE transit_time = 0` in the generated SQL.

Therefore, each incoming question goes to a Retrieval Augmented Generation (RAG) workflow to find potentially relevant stored business information, to enrich the context. This mechanism helps provide the specific rules and interpretations that even advanced LLMs might not be able to derive from general training data.

Parcel Perform uses [Amazon Bedrock Knowledge Bases](https://aws.amazon.com/bedrock/knowledge-bases/) as a managed solution for the RAG workflow. They ingest business context information by uploading files to Amazon S3. Amazon Bedrock Knowledge Bases processes the files, chunks them, uses embedding models to create vectors, and stores the vectors in a vector database so they can be searched. These steps are fully managed by Amazon Bedrock Knowledge Bases. Parcel Perform stores the vectors in [Amazon OpenSearch Serverless](https://aws.amazon.com/opensearch-service/features/serverless/) as the chosen vector database to simplify infrastructure management.

Amazon Bedrock Knowledge Bases provides the [Retrieve API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent-runtime_Retrieve.html), which takes in an input (such as a question from the AI assistant), converts it into a vector embedding, searches for relevant chunks of business context information in the vector database, and returns the top relevant document chunks. It is integrated with the [LangChain](https://www.langchain.com/) Amazon Bedrock Knowledge Bases retriever by [calling the invoke method](https://python.langchain.com/api_reference/aws/retrievers/langchain_aws.retrievers.bedrock.AmazonKnowledgeBasesRetriever.html#langchain_aws.retrievers.bedrock.AmazonKnowledgeBasesRetriever.invoke).

The next step involves invoking an AI agent with the supplied business contextual information and the SQL generation prompt. The prompt was inspired by [a prompt in LangChain Hub](https://smith.langchain.com/hub/langchain-ai/sql-agent-system-prompt). The following is a code snippet of the prompt:

```
You are an agent designed to interact with a SQL database.
Given an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.
Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results.

Relevant context:
{rag_context}

You can order the results by a relevant column to return the most interesting examples in the database.
Never query for all the columns from a specific table, only ask for the relevant columns given the question.
You have access to tools for interacting with the database.
- Only use the below tools. Only use the information returned by the below tools to construct your final answer.
- DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.
- To start querying for final answer you should ALWAYS look at the tables in the database to see what you can query. Do NOT skip this step.
- Then you should query the schema of the most relevant tables
```

The prompt sample is part of the initial instruction for the agent. The data schema is automatically inserted by the tools from the SQLDatabase Toolkit at a later step of this agentic workflow. The following steps occur after a user enters a question in the AI assistant UI:

1. The question triggers a LangGraph execution run.

2. The following processes happen in parallel:
 a. The graph fetches the database schema from Athena through SQLDatabase Toolkit.
 b. The graph passes the question to the Amazon Bedrock Knowledge Bases retriever and gets a list of relevant business information regarding the question.

3. The graph invokes an LLM using Amazon Bedrock by passing the question, the conversation context, data schema, and business context information. The result is the generated SQL.

4. The graph uses the SQLDatabase Toolkit again to run the SQL through Athena and receive data results.

5. The data output is passed into an LLM to generate the final response based on the initial question asked. [Amazon Bedrock Guardrails](https://aws.amazon.com/bedrock/guardrails/) is used as a safeguard to avoid inappropriate inputs and responses.

6. The final response is returned to the user through the AI assistant UI.

The following diagram illustrates these steps.

![Workflow Steps](/images/3-Blog/ML-18476-ai-assistant-architecture-numbered.png)

This implementation demonstrates how Parcel Perform transforms raw inquiries into actionable data for timely decision-making. Security is also implemented in multiple components. From a network perspective, the EKS pods are placed in private subnets in [Amazon Virtual Private Cloud](http://aws.amazon.com/vpc) (Amazon VPC)  to improve network security of the AI assistant application. This AI agent is placed behind a backend layer that requires authentication. For data security, sensitive data is masked at rest in the S3 bucket. Parcel Perform also limits the permissions of the [AWS Identity and Access Management](https://aws.amazon.com/iam/) (IAM) role used to access the S3 bucket so it can only access certain tables.

In the following sections, we discuss how Parcel Perform approached building this data transformation solution.

---

## From idea to production

Parcel Perform started with the idea of freeing their data team from manually serving the request from the business team, while also improving the timeliness of the data availability to support the business team’s decision-making.

With the help of the AWS Solutions Architect team, Parcel Perform completed a proof of concept using AWS services and a [Jupyter notebook](https://jupyter.org/) in [Amazon SageMaker Studio](https://aws.amazon.com/sagemaker-ai/studio/). After an initial success, Parcel Perform integrated the solution with their orchestration tool of choice, LangGraph.

Before going into production, Parcel Perform conducted thorough testing to verify result consistency. They added [LangSmith Tracing](https://docs.smith.langchain.com/observability) to record the steps and results of the AI agent to evaluate its performance.

The Parcel Perform team discovered challenges during their journey, which we discuss in the following section. They performed prompt engineering to address those challenges. Eventually, the AI agent was integrated into production to be used by the business team. Afterward, Parcel Perform collected user feedback internally and monitored logs from LangSmith Tracing to verify performance was maintained.

---

## Challenges

This journey was not immune to challenges.

This journey isn’t free from challenges. Firstly, some ecommerce merchants might have several records in the data lake under various names. For example, a merchant with the name “ABC” might have multiple records such, as “ABC Singapore Holdings Pte. Ltd.,” “ABC Demo Account,” “ABC Test Group,” and so on. For a question like “Was there any parcel shipment delay by ABC last week?”, the generated SQL has the element of `WHERE merchant_name LIKE '%ABC%'` which might result in ambiguity. During the proof of concept stage, this problem caused incorrect matching of the result.

For this challenge, Parcel Perform relies on careful prompt engineering to instruct the LLM to identify when the name was potentially ambiguous. The AI agent then calls Athena again to look for matching names. The LLM decides which merchant name to use based on multiple factors, including the significance in data volume contribution and the account status in the data lake. In the future, Parcel Perform intends to implement a more sophisticated technique by prompting the user to resolve the ambiguity.

The second challenge is about unrestricted questions that might yield expensive queries running across large amounts of data and resulting in longer query waiting time. Some of these questions might not have a LIMIT clause imposed in the query. To solve this, Parcel Perform instructs the LLM to add a LIMIT clause with a certain number of maximum results if the user doesn’t specify the intended number of results. In the future, Parcel Perform plans to use the query EXPLAIN results to identify heavy queries.

The third challenge is related to tracking usage and incurred cost of this particular solution. Having started multiple generative AI projects using Amazon Bedrock and sometimes with the same LLM ID, Parcel Perform must distinguish usage incurred by projects. Parcel Perform creates an [inference profile](https://docs.aws.amazon.com/bedrock/latest/userguide/inference-profiles.html) for each project, associates the profile with [tags](https://docs.aws.amazon.com/whitepapers/latest/tagging-best-practices/what-are-tags.html), and includes that profile in each LLM call for that project. With this setup, Parcel Perform is able to segregate costs based on projects to improve cost visibility and monitoring.

---

## The impact
To extract data, the business team clarifies details with the data team, makes a request, checks feasibility, and waits for bandwidth. This process lengthens when requirements come from customers or teams in different time zones, with each clarification adding 12–24 hours due to asynchronous communication. Simpler requests made early in the workday might complete within 24 hours, whereas more complex requests or those during busy periods can take 3–5 business days.

With the text-to-SQL AI agent, this process is dramatically streamlined—minimizing the back-and-forth communication for requirement clarification, removing the dependency on data team bandwidth, and automating result interpretation.

Parcel Perform’s measurements show that the text-to-SQL AI agent reduces the average time-to-insight by 99%, from 2.3 days to an average of 10 minutes, saving approximately 3,850 total hours of wait time per month across requesters while maintaining data accuracy.

Users can directly query the data without intermediaries, receiving results in minutes rather than days. Teams across time zones can now access insights any time of day, alleviating the frustrating “wait until Asia wakes up” or “catch EMEA before they leave” delays, leading to happier customers and faster problem-solving.

This transformation has profoundly impacted the data analytics team’s capacity and focus, freeing the data team for more strategic work and helping everyone make faster, more informed decisions. Before, the analysts spent approximately 25% of their working hours handling routine data extraction requests—equivalent to over 260 hours monthly across the team. Now, with basic and intermediate queries automated, this number has dropped to just 10%, freeing up nearly 160 hours each month for high-impact work. Analysts now focus on complex data analysis rather than spending time on basic data retrieval tasks.

---

## Conclusion
Parcel Perform’s solution demonstrates how you can use generative AI to enhance productivity and customer experience. Parcel Perform has built a text-to-SQL AI agent that transforms a business team’s question into SQL that can fetch the actual data. This improves the timeliness of data availability for decision-making that involves customers. Furthermore, the data team can avoid the undifferentiated heavy lifting to focus on complex data analysis tasks.

This solution uses multiple AWS services like Amazon Bedrock and tools like LangGraph. You can start with a proof of concept and consult your AWS Solutions Architect or engage with [AWS Partners](https://partners.amazonaws.com/). If you have questions, post them on [AWS re:Post](https://repost.aws/). You can also make the development more straightforward with the help of [Amazon Q Developer](https://aws.amazon.com/q/developer/). When you face challenges, you can iterate to find the solution, which might include prompt engineering or adding additional steps to your workflow.

Security is a top priority. Make sure your AI assistant has proper guardrails in place to protect against prompt threats, inappropriate topics, profanity, leaked data, and other security issues. You can integrate Amazon Bedrock Guardrails with your generative AI application through an API.To learn more, refer to the following resources:

- Build a robust text-to-SQL solution generating complex queries, self-correcting, and querying diverse data sources
- [Xây dựng giải pháp chuyển đổi văn bản sang SQL mạnh mẽ, tạo ra các truy vấn phức tạp, tự động sửa lỗi và truy vấn các nguồn dữ liệu đa dạng](https://aws.amazon.com/blogs/machine-learning/build-a-robust-text-to-sql-solution-generating-complex-queries-self-correcting-and-querying-diverse-data-sources/)
- [LangGraph agents with Amazon Bedrock workshop](https://catalog.us-east-1.prod.workshops.aws/workshops/9bc28f51-d7c3-468b-ba41-72667f3273f1/en-US)
- [Build a knowledge base by connecting to a structured data store](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-build-structured.html)

---

## About the authors

<div style="display: flex; align-items: flex-start; margin-bottom: 30px;">
  <img src="/images/3-Blog/yudho-full2.jpg" alt="Yudho Ahmad Diponegoro" style="width: 150px; height: 150px; object-fit: cover; margin-right: 20px; border-radius: 8px;">
  <div>
    <p><strong>Yudho Ahmad Diponegoro</strong> is a Senior Solutions Architect at AWS. Having been part of Amazon for 10+ years, he has had various roles from software development to solutions architecture. He helps startups in Singapore when it comes to architecting in the cloud. While he keeps his breadth of knowledge across technologies and industries, he focuses in AI and machine learning where he has been guiding various startups in ASEAN to adopt machine learning and generative AI at AWS.</p>
  </div>
</div>

<div style="display: flex; align-items: flex-start; margin-bottom: 30px;">
  <img src="/images/3-Blog/levy-copy-1.png" alt="Le Vy" style="width: 150px; height: 150px; object-fit: cover; margin-right: 20px; border-radius: 8px;">
  <div>
    <p><strong>Le Vy</strong>  is the AI Team Lead at Parcel Perform, where she drives the development of AI applications and explores emerging AI research. She started her career in data analysis and deepened her focus on AI through a Master’s in Artificial Intelligence. Passionate about applying data and AI to solve real business problems, she also dedicates time to mentoring aspiring technologists and building a supportive community for youth in tech. Through her work, Vy actively challenges gender norms in the industry and champions lifelong learning as a key to innovation.</p>
  </div>
</div>

<div style="display: flex; align-items: flex-start; margin-bottom: 30px;">
  <img src="/images/3-Blog/junkai.png" alt="Loke Jun Kai" style="width: 150px; height: 150px; object-fit: cover; margin-right: 20px; border-radius: 8px;">
  <div>
    <p><strong>Loke Jun Kai</strong>  is a GenAI/ML Specialist Solutions Architect in AWS, covering strategic customers across the ASEAN region. He works with customers ranging from Start-up to Enterprise to build cutting-edge use cases and scalable GenAI Platforms. His passion in the AI space, constant research and reading, have led to many innovative solutions built with concrete business outcomes. Outside of work, he enjoys a good game of tennis and chess.</p>
  </div>
</div>